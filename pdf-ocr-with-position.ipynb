{
 "metadata": {
  "kernelspec": {
   "display_name": "Snowflake SQL",
   "language": "snowflake-sql",
   "name": "snowflake-sql"
  },
  "language_info": {
   "name": "snowflake-sql",
   "mimetype": "text/x-sql",
   "file_extension": ".sql"
  },
  "lastEditStatus": {
   "notebookId": "7go4p6stxd27jmqvct4u",
   "authorId": "184210807227",
   "authorName": "AKELKAR",
   "authorEmail": "adwait.kelkar@snowflake.com",
   "sessionId": "151ce0f4-aa26-478d-a03c-016bf92db44b",
   "lastEditTime": 1762818604094
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Phase 0: PDF OCR with Position Tracking - Baseline\n",
    "\n",
    "## Overview\n",
    "This notebook implements the **baseline solution** provided by the Snowflake FCTO for extracting text from PDFs while capturing position information.\n",
    "\n",
    "### What This Does:\n",
    "- Extracts text from PDF documents stored in Snowflake stages\n",
    "- Captures the **x,y coordinates** of each text box on the page\n",
    "- Returns structured data: `{pos: (x,y), txt: text}`\n",
    "\n",
    "### Customer Requirement This Addresses:\n",
    "‚úÖ **Document Intelligence - positioning capability** - knows where text appears on the page\n",
    "\n",
    "### What's Missing (Future Phases):\n",
    "- ‚ùå Page numbers\n",
    "- ‚ùå Section detection\n",
    "- ‚ùå Better chunking\n",
    "- ‚ùå LLM integration\n",
    "- ‚ùå Citation system\n",
    "\n",
    "---\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Set up the Snowflake environment with appropriate roles and context.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell3",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Use administrative role to grant permissions\n",
    "USE ROLE accountadmin;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell4",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Grant access to PyPI packages (needed for pdfminer library)\nGRANT DATABASE ROLE SNOWFLAKE.PYPI_REPOSITORY_USER TO ROLE accountadmin;\n",
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## Step 2: Database and Schema Setup\n",
    "\n",
    "Create the PDF_OCR schema in the SANDBOX database for this project.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "cell7",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Create the PDF_OCR schema if it doesn't exist\n",
    "CREATE SCHEMA IF NOT EXISTS SANDBOX.PDF_OCR\n",
    "COMMENT = 'Schema for PDF OCR with position tracking solution';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell8",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Set database and schema context\n",
    "USE DATABASE SANDBOX;\n",
    "USE SCHEMA PDF_OCR;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "## Step 3: Create Stage for PDF Storage\n",
    "\n",
    "Stages in Snowflake are locations where data files are stored. We'll create an internal stage to hold our PDF documents.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell10",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create internal stage for PDF files\n",
    "CREATE STAGE IF NOT EXISTS PDF_STAGE\n",
    "COMMENT = 'Stage for storing clinical protocol PDFs and other documents';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell11",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Verify stage was created\n",
    "SHOW STAGES LIKE 'PDF_STAGE';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## Step 4: Create PDF Text Mapper UDF\n",
    "\n",
    "This User-Defined Function (UDF) is the core of our solution. Let's break down what it does:\n",
    "\n",
    "### Technology Stack:\n",
    "- **Language:** Python 3.12\n",
    "- **Library:** `pdfminer` - A robust PDF parsing library\n",
    "- **Snowflake Integration:** Uses `SnowflakeFile` to read directly from stages\n",
    "\n",
    "### How It Works:\n",
    "1. Opens the PDF file from the Snowflake stage\n",
    "2. Iterates through each page\n",
    "3. Extracts text boxes (`LTTextBox` objects) from the page layout\n",
    "4. Captures the **bounding box coordinates** (bbox) - specifically:\n",
    "   - `bbox[0]` = x-coordinate (left)\n",
    "   - `bbox[3]` = y-coordinate (top)\n",
    "5. Returns an array of objects: `{pos: (x,y), txt: text}`\n",
    "\n",
    "### Input:\n",
    "- `scoped_file_url`: A Snowflake-generated URL pointing to a file in a stage\n",
    "\n",
    "### Output:\n",
    "- VARCHAR (JSON string) containing array of text boxes with positions\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell13",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION pdf_txt_mapper(scoped_file_url string)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pdfminer')\n",
    "HANDLER = 'main'\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "def main(scoped_file_url):\n",
    "    finding = []\n",
    "    with SnowflakeFile.open(scoped_file_url, 'rb') as f:\n",
    "        # Initialize PDF processing components\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()  # Layout analysis parameters\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        pages = PDFPage.get_pages(f)\n",
    "        \n",
    "        # Process each page\n",
    "        for page in pages:\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            \n",
    "            # Extract text boxes from the page\n",
    "            for lobj in layout:\n",
    "                if isinstance(lobj, LTTextBox):\n",
    "                    # bbox = (x0, y0, x1, y1) where (x0,y0) is bottom-left, (x1,y1) is top-right\n",
    "                    x, y, text = lobj.bbox[0], lobj.bbox[3], lobj.get_text()\n",
    "                    finding += [{'pos': (x, y), 'txt': text}]\n",
    "    \n",
    "    return str(finding)\n",
    "$$;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell14",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Verify function was created\n",
    "SHOW FUNCTIONS LIKE 'pdf_txt_mapper';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "## Step 5: Upload PDF to Stage\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Option 1: Using Snowflake Web UI**\n",
    "1. Navigate to Data ‚Üí Databases ‚Üí SANDBOX ‚Üí PDF_OCR ‚Üí Stages\n",
    "2. Click on the `PDF_STAGE` stage\n",
    "3. Click \"+ Files\" button in the top right\n",
    "4. Upload your PDF file (e.g., `Prot_000.pdf`)\n",
    "\n",
    "**Option 2: Using SnowSQL CLI**\n",
    "```bash\n",
    "snowsql -a <account> -u <username>\n",
    "USE SCHEMA SANDBOX.PDF_OCR;\n",
    "PUT file:///path/to/your/file.pdf @PDF_STAGE AUTO_COMPRESS=FALSE;\n",
    "```\n",
    "\n",
    "**Option 3: Using Python Snowpark**\n",
    "```python\n",
    "session.file.put(\"Prot_000.pdf\", \"@PDF_STAGE\", auto_compress=False)\n",
    "```\n",
    "\n",
    "Let's verify the file after upload:\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell16",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- List files in the PDF stage\n",
    "LIST @PDF_STAGE;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "## Step 6: Test the PDF Text Mapper\n",
    "\n",
    "Now let's test our function with the uploaded PDF.\n",
    "\n",
    "### What to Expect:\n",
    "- The function will return a VARCHAR (string representation of a Python list)\n",
    "- Each element will be: `{'pos': (x, y), 'txt': 'extracted text'}`\n",
    "- The output will be **very long** for multi-page documents\n",
    "\n",
    "### Note on `build_scoped_file_url()`:\n",
    "This Snowflake function generates a temporary, scoped URL that allows the UDF to securely access the staged file.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell18",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Test with the clinical protocol PDF\n",
    "-- This will return the full extracted text with positions\n",
    "SELECT pdf_txt_mapper(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf')) AS extracted_data;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "## Step 7: Analyze the Output\n",
    "\n",
    "Let's get some basic statistics about what was extracted.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell20",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Get the length of the output\n",
    "SELECT \n",
    "    LENGTH(pdf_txt_mapper(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf'))) AS output_length_chars,\n",
    "    LENGTH(pdf_txt_mapper(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf'))) / 1024 AS output_length_kb;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "## Phase 0 Summary\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "1. Set up Snowflake environment with proper roles and permissions\n",
    "2. Created a stage for storing PDF documents\n",
    "3. Deployed the FCTO's baseline PDF text mapper UDF\n",
    "4. Extracted text from a clinical protocol PDF with position information\n",
    "\n",
    "### üìä Current Output Format:\n",
    "```python\n",
    "[{'pos': (54.0, 720.3), 'txt': 'CLINICAL PROTOCOL\\n'}, \n",
    " {'pos': (72.0, 680.1), 'txt': 'Study Title: ...\\n'},\n",
    " ...]\n",
    "```\n",
    "\n",
    "### üéØ What This Gives Us:\n",
    "- ‚úÖ Text extraction from PDFs\n",
    "- ‚úÖ X,Y coordinates for each text box\n",
    "- ‚úÖ Snowflake-native processing (no external services)\n",
    "\n",
    "### ‚ö†Ô∏è Current Limitations:\n",
    "- ‚ùå No page number information\n",
    "- ‚ùå No section/hierarchy detection\n",
    "- ‚ùå Text boxes may be too granular or broken\n",
    "- ‚ùå Output is a string, not structured data we can query\n",
    "- ‚ùå No way to answer \"Where did this info come from?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Phase 1\n",
    "In the next phase, we'll enhance this solution to:\n",
    "1. **Add page numbers** to each text box\n",
    "2. Store results in a **queryable table** instead of a string\n",
    "3. Add a **unique chunk ID** for each text box\n",
    "\n",
    "This will enable queries like:\n",
    "```sql\n",
    "SELECT * FROM document_chunks \n",
    "WHERE page = 5 \n",
    "AND txt ILIKE '%medication%';\n",
    "```\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell22"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Permission Error on PyPI:**\n",
    "```\n",
    "Error: Access denied for database role SNOWFLAKE.PYPI_REPOSITORY_USER\n",
    "```\n",
    "**Solution:** Make sure you ran the GRANT command as ACCOUNTADMIN\n",
    "\n",
    "**2. File Not Found:**\n",
    "```\n",
    "Error: File 'Prot_000.pdf' does not exist\n",
    "```\n",
    "**Solution:** Verify the file was uploaded with `LIST @PDF_STAGE;`\n",
    "\n",
    "**3. Function Takes Too Long:**\n",
    "- Large PDFs (100+ pages) can take 30-60 seconds\n",
    "- This is normal for the initial processing\n",
    "- Consider processing in batches for very large documents\n",
    "\n",
    "**4. Memory Issues:**\n",
    "- For very large PDFs (500+ pages), you may need to increase warehouse size\n",
    "- Or split the PDF into smaller chunks before processing\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Add Page Numbers & Structured Storage\n",
    "\n",
    "## What We're Adding\n",
    "\n",
    "In Phase 1, we'll enhance the baseline solution with:\n",
    "1. **Page number tracking** - Know which page each text box came from\n",
    "2. **Table storage** - Store results in a queryable table (not VARCHAR)\n",
    "3. **Chunk IDs** - Unique identifiers for each text box\n",
    "4. **Timestamps** - Track when documents were processed\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ Query specific pages: `WHERE page = 5`\n",
    "- ‚úÖ Search across documents: `WHERE text ILIKE '%medication%'`\n",
    "- ‚úÖ Audit trail: When was this document processed?\n",
    "- ‚úÖ Compare multiple PDFs in the same table\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step1"
   },
   "source": [
    "## Step 1: Create Document Chunks Table\n",
    "\n",
    "This table will store the extracted text with metadata:\n",
    "- `chunk_id`: Unique identifier (e.g., 'Prot_000_p5_c42')\n",
    "- `doc_name`: Source PDF filename\n",
    "- `page`: Page number (1-indexed)\n",
    "- `x, y`: Position coordinates\n",
    "- `text`: Extracted text content\n",
    "- `extracted_at`: Timestamp of extraction\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100001"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_create_table",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE document_chunks (\n",
    "    chunk_id VARCHAR PRIMARY KEY,\n",
    "    doc_name VARCHAR NOT NULL,\n",
    "    page INTEGER NOT NULL,\n",
    "    x FLOAT,\n",
    "    y FLOAT,\n",
    "    text VARCHAR,\n",
    "    extracted_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100002"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_verify_table",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Verify table was created\n",
    "DESC TABLE document_chunks;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step2"
   },
   "source": [
    "## Step 2: Enhanced UDF with Page Numbers\n",
    "\n",
    "Now we'll create an **enhanced version** of the UDF that tracks page numbers.\n",
    "\n",
    "### Key Changes:\n",
    "1. `enumerate(pages, start=1)` - Track page numbers starting from 1\n",
    "2. `'page': page_num` - Include page number in output\n",
    "3. Returns JSON with page information\n",
    "\n",
    "### Output Format:\n",
    "```python\n",
    "[{'page': 1, 'pos': (54.0, 720.3), 'txt': 'CLINICAL PROTOCOL'},\n",
    " {'page': 1, 'pos': (72.0, 680.1), 'txt': 'Study Title: ...'},\n",
    " {'page': 2, 'pos': (54.0, 720.3), 'txt': 'Section 1: ...'}]\n",
    "```\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100004"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_enhanced_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION pdf_txt_mapper_v2(scoped_file_url string)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pdfminer')\n",
    "HANDLER = 'main'\n",
    "AS\n",
    "$$\n",
    "import json\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "def main(scoped_file_url):\n",
    "    finding = []\n",
    "    with SnowflakeFile.open(scoped_file_url, 'rb') as f:\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        pages = PDFPage.get_pages(f)\n",
    "        \n",
    "        # Track page numbers with enumerate\n",
    "        for page_num, page in enumerate(pages, start=1):\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            \n",
    "            for lobj in layout:\n",
    "                if isinstance(lobj, LTTextBox):\n",
    "                    x, y, text = lobj.bbox[0], lobj.bbox[3], lobj.get_text()\n",
    "                    # Use list [x, y] instead of tuple (x, y) for valid JSON\n",
    "                    finding.append({\n",
    "                        'page': page_num,\n",
    "                        'pos': [x, y],\n",
    "                        'txt': text\n",
    "                    })\n",
    "    \n",
    "    # Return valid JSON using json.dumps()\n",
    "    return json.dumps(finding)\n",
    "$$;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100005"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_verify_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Verify the enhanced function was created\n",
    "SHOW FUNCTIONS LIKE 'pdf_txt_mapper_v2';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step3"
   },
   "source": [
    "## Step 3: Test Enhanced UDF\n",
    "\n",
    "Let's test the new UDF to verify it now includes page numbers.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100007"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_test_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Test the enhanced UDF - should now include page numbers\n",
    "SELECT pdf_txt_mapper_v2(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf')) AS extracted_data_with_pages;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step4"
   },
   "source": [
    "## Step 4: Parse and Load Data into Table\n",
    "\n",
    "Now we'll parse the JSON output and load it into our `document_chunks` table.\n",
    "\n",
    "We'll use Snowflake's JSON parsing functions:\n",
    "- `PARSE_JSON()` - Parse the VARCHAR into JSON\n",
    "- `FLATTEN()` - Convert JSON array into rows\n",
    "- `GET()` - Extract specific fields from JSON objects\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100009"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_load_data",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Parse JSON and insert into table\n",
    "INSERT INTO document_chunks (chunk_id, doc_name, page, x, y, text)\n",
    "SELECT \n",
    "    'Prot_000_p' || value:page || '_c' || ROW_NUMBER() OVER (ORDER BY value:page, value:pos[0], value:pos[1]) AS chunk_id,\n",
    "    'Prot_000.pdf' AS doc_name,\n",
    "    value:page::INTEGER AS page,\n",
    "    value:pos[0]::FLOAT AS x,\n",
    "    value:pos[1]::FLOAT AS y,\n",
    "    value:txt::VARCHAR AS text\n",
    "FROM (\n",
    "    SELECT PARSE_JSON(pdf_txt_mapper_v2(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf'))) AS parsed_data\n",
    "),\n",
    "LATERAL FLATTEN(input => parsed_data) AS f;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step5"
   },
   "source": [
    "## Step 5: Query the Results!\n",
    "\n",
    "Now we can query the extracted data using SQL. This is the **power of Phase 1** - structured, queryable data!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100011"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_count_chunks",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- How many text chunks were extracted?\n",
    "SELECT COUNT(*) AS total_chunks FROM document_chunks;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100012"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_chunks_per_page",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- How many chunks per page?\n",
    "SELECT \n",
    "    page,\n",
    "    COUNT(*) AS chunks_on_page\n",
    "FROM document_chunks\n",
    "GROUP BY page\n",
    "ORDER BY page\n",
    "LIMIT 20;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100013"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_search_medication",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Search for mentions of 'medication' or 'drug'\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    page,\n",
    "    SUBSTR(text, 1, 100) AS text_preview\n",
    "FROM document_chunks\n",
    "WHERE text ILIKE '%medication%'\n",
    "   OR text ILIKE '%drug%'\n",
    "ORDER BY page\n",
    "LIMIT 10;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100014"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_specific_page",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Get all text from a specific page (e.g., page 5)\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    x,\n",
    "    y,\n",
    "    text\n",
    "FROM document_chunks\n",
    "WHERE page = 5\n",
    "ORDER BY y DESC, x;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_summary"
   },
   "source": [
    "## Phase 1 Summary\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "1. Created `document_chunks` table for structured storage\n",
    "2. Enhanced UDF (`pdf_txt_mapper_v2`) with page number tracking\n",
    "3. Parsed JSON output and loaded into queryable table\n",
    "4. Demonstrated SQL queries on extracted text\n",
    "\n",
    "### üìä New Capabilities:\n",
    "```sql\n",
    "-- Query by page\n",
    "SELECT * FROM document_chunks WHERE page = 5;\n",
    "\n",
    "-- Search for keywords\n",
    "SELECT * FROM document_chunks WHERE text ILIKE '%medication%';\n",
    "\n",
    "-- Count chunks per page\n",
    "SELECT page, COUNT(*) FROM document_chunks GROUP BY page;\n",
    "```\n",
    "\n",
    "### üéØ What This Gives Us:\n",
    "- ‚úÖ **Page numbers** - Know which page every text box came from\n",
    "- ‚úÖ **Queryable data** - Use SQL instead of parsing strings\n",
    "- ‚úÖ **Chunk IDs** - Unique identifiers for traceability\n",
    "- ‚úÖ **Timestamps** - Track when documents were processed\n",
    "- ‚úÖ **Citation foundation** - Can now answer \"This is on page 5\"\n",
    "\n",
    "### ‚ö†Ô∏è Still Missing (Future Phases):\n",
    "- ‚ùå Full bounding boxes (only have x,y corner) ‚Üí Phase 2\n",
    "- ‚ùå Font information (size, bold/italic) ‚Üí Phase 3\n",
    "- ‚ùå Section detection (headers, hierarchy) ‚Üí Phase 4\n",
    "- ‚ùå Smart chunking (semantic boundaries) ‚Üí Phase 5\n",
    "- ‚ùå LLM integration with citations ‚Üí Phase 6\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Phase 2\n",
    "In Phase 2, we'll capture **full bounding boxes** (x0, y0, x1, y1) instead of just (x, y). This will enable:\n",
    "- Highlighting text in PDF viewers\n",
    "- Detecting multi-column layouts\n",
    "- Calculating text height/width\n",
    "- More accurate positioning for citations\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100016"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Phase 2: Full Bounding Boxes\n",
    "\n",
    "## What We're Adding\n",
    "\n",
    "In Phase 2, we'll enhance the solution to capture **complete rectangles** instead of just corner points:\n",
    "1. **Full bounding boxes** - (x0, y0, x1, y1) instead of just (x, y)\n",
    "2. **Page dimensions** - Width and height of each page\n",
    "3. **Text dimensions** - Calculate width and height of text boxes\n",
    "4. **Visual highlighting** - Enable PDF viewer highlighting\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ Draw rectangles around extracted text in PDF viewers\n",
    "- ‚úÖ Calculate relative positions (% from top/left)\n",
    "- ‚úÖ Detect multi-column layouts\n",
    "- ‚úÖ Measure text width and height\n",
    "- ‚úÖ Enable visual highlighting in Streamlit apps\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_step1"
   },
   "source": [
    "## Step 1: Update Table Schema\n",
    "\n",
    "We'll alter the existing table to add full bounding box columns.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200001"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_alter_table",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Add bounding box columns to existing table\n",
    "ALTER TABLE document_chunks ADD COLUMN IF NOT EXISTS bbox_x0 FLOAT;\n",
    "ALTER TABLE document_chunks ADD COLUMN IF NOT EXISTS bbox_y0 FLOAT;\n",
    "ALTER TABLE document_chunks ADD COLUMN IF NOT EXISTS bbox_x1 FLOAT;\n",
    "ALTER TABLE document_chunks ADD COLUMN IF NOT EXISTS bbox_y1 FLOAT;\n",
    "ALTER TABLE document_chunks ADD COLUMN IF NOT EXISTS page_width FLOAT;\n",
    "ALTER TABLE document_chunks ADD COLUMN IF NOT EXISTS page_height FLOAT;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200002"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_verify_schema",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Verify new columns were added\n",
    "DESC TABLE document_chunks;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_step2"
   },
   "source": [
    "## Step 2: Enhanced UDF with Full Bounding Boxes\n",
    "\n",
    "Now we'll create a new version of the UDF that captures the **complete bounding box**.\n",
    "\n",
    "### Key Changes:\n",
    "1. `x0, y0, x1, y1 = lobj.bbox` - Capture all 4 corners\n",
    "2. `page.width, page.height` - Capture page dimensions\n",
    "3. Returns complete rectangle coordinates\n",
    "\n",
    "### Bounding Box Explained:\n",
    "```\n",
    "(x0, y1)  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ   Text Box   ‚îÇ\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  (x1, y0)\n",
    "```\n",
    "- `x0, y0` = Bottom-left corner\n",
    "- `x1, y1` = Top-right corner\n",
    "- PDF coordinates start at bottom-left (0,0)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200004"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_enhanced_udf",
    "language": "python"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION pdf_txt_mapper_v3(scoped_file_url string)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pdfminer')\n",
    "HANDLER = 'main'\n",
    "AS\n",
    "$$\n",
    "import json\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "def main(scoped_file_url):\n",
    "    finding = []\n",
    "    with SnowflakeFile.open(scoped_file_url, 'rb') as f:\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        pages = PDFPage.get_pages(f)\n",
    "        \n",
    "        # Track page numbers\n",
    "        for page_num, page in enumerate(pages, start=1):\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            \n",
    "            # Get page dimensions\n",
    "            page_width = layout.width\n",
    "            page_height = layout.height\n",
    "            \n",
    "            for lobj in layout:\n",
    "                if isinstance(lobj, LTTextBox):\n",
    "                    # NEW: Capture FULL bounding box (all 4 corners)\n",
    "                    x0, y0, x1, y1 = lobj.bbox\n",
    "                    text = lobj.get_text()\n",
    "                    \n",
    "                    finding.append({\n",
    "                        'page': page_num,\n",
    "                        'bbox': [x0, y0, x1, y1],  # Full rectangle!\n",
    "                        'page_width': page_width,\n",
    "                        'page_height': page_height,\n",
    "                        'txt': text\n",
    "                    })\n",
    "    \n",
    "    return json.dumps(finding)\n",
    "$$;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200005"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_verify_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Verify the enhanced function was created\n",
    "SHOW FUNCTIONS LIKE 'pdf_txt_mapper_v3';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_step3"
   },
   "source": [
    "## Step 3: Test Enhanced UDF\n",
    "\n",
    "Let's test the new UDF to verify it captures full bounding boxes.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200007"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_test_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Test the enhanced UDF - should now include full bounding boxes\n",
    "SELECT pdf_txt_mapper_v3(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf')) AS extracted_data_with_bbox;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_step4"
   },
   "source": [
    "## Step 4: Clear Old Data and Load with Full Bbox\n",
    "\n",
    "We'll truncate the table and reload with the enhanced data including full bounding boxes.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200009"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_truncate",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Clear existing data (optional - comment out if you want to keep Phase 1 data)\n",
    "TRUNCATE TABLE document_chunks;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200010"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_load_data",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Parse JSON and insert with full bounding box data\n",
    "INSERT INTO document_chunks (\n",
    "    chunk_id, doc_name, page, \n",
    "    x, y,  -- Keep old columns for backward compatibility\n",
    "    bbox_x0, bbox_y0, bbox_x1, bbox_y1,  -- New: Full bbox\n",
    "    page_width, page_height,              -- New: Page dimensions\n",
    "    text\n",
    ")\n",
    "SELECT \n",
    "    'Prot_000_p' || value:page || '_c' || ROW_NUMBER() OVER (ORDER BY value:page, value:bbox[0], value:bbox[1]) AS chunk_id,\n",
    "    'Prot_000.pdf' AS doc_name,\n",
    "    value:page::INTEGER AS page,\n",
    "    value:bbox[0]::FLOAT AS x,          -- Top-left x (for compatibility)\n",
    "    value:bbox[3]::FLOAT AS y,          -- Top-left y (for compatibility)\n",
    "    value:bbox[0]::FLOAT AS bbox_x0,    -- Bottom-left x\n",
    "    value:bbox[1]::FLOAT AS bbox_y0,    -- Bottom-left y\n",
    "    value:bbox[2]::FLOAT AS bbox_x1,    -- Top-right x\n",
    "    value:bbox[3]::FLOAT AS bbox_y1,    -- Top-right y\n",
    "    value:page_width::FLOAT AS page_width,\n",
    "    value:page_height::FLOAT AS page_height,\n",
    "    value:txt::VARCHAR AS text\n",
    "FROM (\n",
    "    SELECT PARSE_JSON(pdf_txt_mapper_v3(build_scoped_file_url(@PDF_STAGE, 'Prot_000.pdf'))) AS parsed_data\n",
    "),\n",
    "LATERAL FLATTEN(input => parsed_data) AS f;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200011"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_step5"
   },
   "source": [
    "## Step 5: Query with Bounding Box Data\n",
    "\n",
    "Now we can use the full bounding box information for advanced queries.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200012"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_text_dimensions",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Calculate text box dimensions\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    page,\n",
    "    (bbox_x1 - bbox_x0) AS width,\n",
    "    (bbox_y1 - bbox_y0) AS height,\n",
    "    SUBSTR(text, 1, 50) AS text_preview\n",
    "FROM document_chunks\n",
    "ORDER BY height DESC\n",
    "LIMIT 10;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200013"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_relative_position",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Calculate relative positions (useful for detecting headers)\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    page,\n",
    "    ROUND((bbox_x0 / page_width) * 100, 1) AS left_percent,\n",
    "    ROUND((bbox_y0 / page_height) * 100, 1) AS bottom_percent,\n",
    "    SUBSTR(text, 1, 50) AS text_preview\n",
    "FROM document_chunks\n",
    "WHERE (bbox_y0 / page_height) > 0.8  -- Top 20% of page (likely headers)\n",
    "ORDER BY page\n",
    "LIMIT 10;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200014"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_column_detection",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Detect multi-column layouts\n",
    "SELECT \n",
    "    page,\n",
    "    CASE \n",
    "        WHEN bbox_x0 < page_width/2 THEN 'LEFT_COLUMN'\n",
    "        ELSE 'RIGHT_COLUMN'\n",
    "    END AS column,\n",
    "    COUNT(*) as text_boxes\n",
    "FROM document_chunks\n",
    "GROUP BY page, column\n",
    "ORDER BY page;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200015"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase2_citation_with_bbox",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Get citations with full bbox for visual highlighting\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    page,\n",
    "    bbox_x0,\n",
    "    bbox_y0,\n",
    "    bbox_x1,\n",
    "    bbox_y1,\n",
    "    SUBSTR(text, 1, 100) AS text_preview\n",
    "FROM document_chunks\n",
    "WHERE text ILIKE '%medication%'\n",
    "ORDER BY page\n",
    "LIMIT 5;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200016"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase2_summary"
   },
   "source": [
    "## Phase 2 Summary\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "1. Added full bounding box columns to `document_chunks` table\n",
    "2. Created enhanced UDF (`pdf_txt_mapper_v3`) that captures complete rectangles\n",
    "3. Loaded data with full bbox coordinates (x0, y0, x1, y1)\n",
    "4. Added page dimensions (width, height)\n",
    "5. Demonstrated advanced queries using bbox data\n",
    "\n",
    "### üìä New Capabilities:\n",
    "```sql\n",
    "-- Calculate text dimensions\n",
    "SELECT (bbox_x1 - bbox_x0) AS width, (bbox_y1 - bbox_y0) AS height;\n",
    "\n",
    "-- Find headers (top of page)\n",
    "SELECT * WHERE (bbox_y0 / page_height) > 0.8;\n",
    "\n",
    "-- Detect columns\n",
    "SELECT CASE WHEN bbox_x0 < page_width/2 THEN 'LEFT' ELSE 'RIGHT' END;\n",
    "```\n",
    "\n",
    "### üéØ What This Enables:\n",
    "- ‚úÖ **Visual highlighting** in PDF viewers (Streamlit app!)\n",
    "- ‚úÖ **Text dimensions** for header detection\n",
    "- ‚úÖ **Relative positioning** for layout analysis\n",
    "- ‚úÖ **Column detection** for multi-column documents\n",
    "- ‚úÖ **Precise citations** with exact rectangles\n",
    "\n",
    "### üí° Use with Streamlit App:\n",
    "The `streamlit_pdf_viewer.py` app can now:\n",
    "1. Query chunks with full bbox data\n",
    "2. Draw highlight rectangles on PDF pages\n",
    "3. Show exact location visually\n",
    "4. Enable \"click to highlight\" functionality\n",
    "\n",
    "### ‚ö†Ô∏è Still Missing (Future Phases):\n",
    "- ‚ùå Font information (size, bold/italic) ‚Üí Phase 3\n",
    "- ‚ùå Section detection (headers, hierarchy) ‚Üí Phase 4\n",
    "- ‚ùå Smart chunking (semantic boundaries) ‚Üí Phase 5\n",
    "- ‚ùå LLM integration with citations ‚Üí Phase 6\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Phase 3\n",
    "In Phase 3, we'll extract **font information** (name, size, bold/italic) to automatically detect headers and section boundaries.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff200017"
  }
 ]
}
