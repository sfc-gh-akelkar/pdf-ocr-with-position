{
 "metadata": {
  "kernelspec": {
   "display_name": "Snowflake SQL",
   "language": "snowflake-sql",
   "name": "snowflake-sql"
  },
  "language_info": {
   "name": "snowflake-sql",
   "mimetype": "text/x-sql",
   "file_extension": ".sql"
  },
  "lastEditStatus": {
   "notebookId": "yr5qfmddxg26wpuxylty",
   "authorId": "184210807227",
   "authorName": "AKELKAR",
   "authorEmail": "adwait.kelkar@snowflake.com",
   "sessionId": "db40e462-f970-4bbf-80e9-6d739cc05a1d",
   "lastEditTime": 1760734648334
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Phase 0: PDF OCR with Position Tracking - Baseline\n",
    "\n",
    "## Overview\n",
    "This notebook implements the **baseline solution** provided by the Snowflake FCTO for extracting text from PDFs while capturing position information.\n",
    "\n",
    "### What This Does:\n",
    "- Extracts text from PDF documents stored in Snowflake stages\n",
    "- Captures the **x,y coordinates** of each text box on the page\n",
    "- Returns structured data: `{pos: (x,y), txt: text}`\n",
    "\n",
    "### Customer Requirement This Addresses:\n",
    "‚úÖ **Document Intelligence - positioning capability** - knows where text appears on the page\n",
    "\n",
    "### What's Missing (Future Phases):\n",
    "- ‚ùå Page numbers\n",
    "- ‚ùå Section detection\n",
    "- ‚ùå Better chunking\n",
    "- ‚ùå LLM integration\n",
    "- ‚ùå Citation system\n",
    "\n",
    "---\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Set up the Snowflake environment with appropriate roles and context.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell3",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Use administrative role to grant permissions\n",
    "USE ROLE accountadmin;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell4",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Grant access to PyPI packages (needed for pdfminer library)\nGRANT DATABASE ROLE SNOWFLAKE.PYPI_REPOSITORY_USER TO ROLE accountadmin;\n",
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## Step 2: Database and Schema Setup\n",
    "\n",
    "Create or use an existing database and schema for this project.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell8",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create schema if it doesn't exist\n",
    "USE DATABASE SANDBOX;\n",
    "USE SCHEMA PUBLIC;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "## Step 3: Create Stage for PDF Storage\n",
    "\n",
    "Stages in Snowflake are locations where data files are stored. We'll create an internal stage to hold our PDF documents.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell10",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Create internal stage for PDF files\n",
    "CREATE STAGE IF NOT EXISTS pdf\n",
    "COMMENT = 'Stage for storing clinical protocol PDFs and other documents';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell11",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Verify stage was created\n",
    "SHOW STAGES LIKE 'pdf';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## Step 4: Create PDF Text Mapper UDF\n",
    "\n",
    "This User-Defined Function (UDF) is the core of our solution. Let's break down what it does:\n",
    "\n",
    "### Technology Stack:\n",
    "- **Language:** Python 3.12\n",
    "- **Library:** `pdfminer` - A robust PDF parsing library\n",
    "- **Snowflake Integration:** Uses `SnowflakeFile` to read directly from stages\n",
    "\n",
    "### How It Works:\n",
    "1. Opens the PDF file from the Snowflake stage\n",
    "2. Iterates through each page\n",
    "3. Extracts text boxes (`LTTextBox` objects) from the page layout\n",
    "4. Captures the **bounding box coordinates** (bbox) - specifically:\n",
    "   - `bbox[0]` = x-coordinate (left)\n",
    "   - `bbox[3]` = y-coordinate (top)\n",
    "5. Returns an array of objects: `{pos: (x,y), txt: text}`\n",
    "\n",
    "### Input:\n",
    "- `scoped_file_url`: A Snowflake-generated URL pointing to a file in a stage\n",
    "\n",
    "### Output:\n",
    "- VARCHAR (JSON string) containing array of text boxes with positions\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell13",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION pdf_txt_mapper(scoped_file_url string)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pdfminer')\n",
    "HANDLER = 'main'\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "def main(scoped_file_url):\n",
    "    finding = []\n",
    "    with SnowflakeFile.open(scoped_file_url, 'rb') as f:\n",
    "        # Initialize PDF processing components\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()  # Layout analysis parameters\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        pages = PDFPage.get_pages(f)\n",
    "        \n",
    "        # Process each page\n",
    "        for page in pages:\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            \n",
    "            # Extract text boxes from the page\n",
    "            for lobj in layout:\n",
    "                if isinstance(lobj, LTTextBox):\n",
    "                    # bbox = (x0, y0, x1, y1) where (x0,y0) is bottom-left, (x1,y1) is top-right\n",
    "                    x, y, text = lobj.bbox[0], lobj.bbox[3], lobj.get_text()\n",
    "                    finding += [{'pos': (x, y), 'txt': text}]\n",
    "    \n",
    "    return str(finding)\n",
    "$$;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell14",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Verify function was created\n",
    "SHOW FUNCTIONS LIKE 'pdf_txt_mapper';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "## Step 5: Upload PDF to Stage\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Option 1: Using SnowSQL CLI**\n",
    "```bash\n",
    "snowsql -a <account> -u <username>\n",
    "PUT file:///Users/akelkar/src/Cursor/pdf-ocr-with-position/Prot_000.pdf @pdf_ocr_demo.public.pdf AUTO_COMPRESS=FALSE;\n",
    "```\n",
    "\n",
    "**Option 2: Using Snowflake Web UI**\n",
    "1. Navigate to Data ‚Üí Databases ‚Üí PDF_OCR_DEMO ‚Üí PUBLIC ‚Üí Stages\n",
    "2. Click on the `PDF` stage\n",
    "3. Click \"+ Files\" button in the top right\n",
    "4. Upload `Prot_000.pdf`\n",
    "\n",
    "**Option 3: Using Python Snowpark**\n",
    "```python\n",
    "session.file.put(\"Prot_000.pdf\", \"@pdf\", auto_compress=False)\n",
    "```\n",
    "\n",
    "Let's verify the file after upload:\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell16",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- List files in the PDF stage\n",
    "LIST @pdf;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "## Step 6: Test the PDF Text Mapper\n",
    "\n",
    "Now let's test our function with the uploaded PDF.\n",
    "\n",
    "### What to Expect:\n",
    "- The function will return a VARCHAR (string representation of a Python list)\n",
    "- Each element will be: `{'pos': (x, y), 'txt': 'extracted text'}`\n",
    "- The output will be **very long** for multi-page documents\n",
    "\n",
    "### Note on `build_scoped_file_url()`:\n",
    "This Snowflake function generates a temporary, scoped URL that allows the UDF to securely access the staged file.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell18",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Test with the clinical protocol PDF\n",
    "-- This will return the full extracted text with positions\n",
    "SELECT pdf_txt_mapper(build_scoped_file_url(@pdf, 'Prot_000.pdf')) AS extracted_data;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "## Step 7: Analyze the Output\n",
    "\n",
    "Let's get some basic statistics about what was extracted.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell20",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Get the length of the output\n",
    "SELECT \n",
    "    LENGTH(pdf_txt_mapper(build_scoped_file_url(@pdf, 'Prot_000.pdf'))) AS output_length_chars,\n",
    "    LENGTH(pdf_txt_mapper(build_scoped_file_url(@pdf, 'Prot_000.pdf'))) / 1024 AS output_length_kb;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "## Phase 0 Summary\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "1. Set up Snowflake environment with proper roles and permissions\n",
    "2. Created a stage for storing PDF documents\n",
    "3. Deployed the FCTO's baseline PDF text mapper UDF\n",
    "4. Extracted text from a clinical protocol PDF with position information\n",
    "\n",
    "### üìä Current Output Format:\n",
    "```python\n",
    "[{'pos': (54.0, 720.3), 'txt': 'CLINICAL PROTOCOL\\n'}, \n",
    " {'pos': (72.0, 680.1), 'txt': 'Study Title: ...\\n'},\n",
    " ...]\n",
    "```\n",
    "\n",
    "### üéØ What This Gives Us:\n",
    "- ‚úÖ Text extraction from PDFs\n",
    "- ‚úÖ X,Y coordinates for each text box\n",
    "- ‚úÖ Snowflake-native processing (no external services)\n",
    "\n",
    "### ‚ö†Ô∏è Current Limitations:\n",
    "- ‚ùå No page number information\n",
    "- ‚ùå No section/hierarchy detection\n",
    "- ‚ùå Text boxes may be too granular or broken\n",
    "- ‚ùå Output is a string, not structured data we can query\n",
    "- ‚ùå No way to answer \"Where did this info come from?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Phase 1\n",
    "In the next phase, we'll enhance this solution to:\n",
    "1. **Add page numbers** to each text box\n",
    "2. Store results in a **queryable table** instead of a string\n",
    "3. Add a **unique chunk ID** for each text box\n",
    "\n",
    "This will enable queries like:\n",
    "```sql\n",
    "SELECT * FROM document_chunks \n",
    "WHERE page = 5 \n",
    "AND txt ILIKE '%medication%';\n",
    "```\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell22"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Permission Error on PyPI:**\n",
    "```\n",
    "Error: Access denied for database role SNOWFLAKE.PYPI_REPOSITORY_USER\n",
    "```\n",
    "**Solution:** Make sure you ran the GRANT command as ACCOUNTADMIN\n",
    "\n",
    "**2. File Not Found:**\n",
    "```\n",
    "Error: File 'Prot_000.pdf' does not exist\n",
    "```\n",
    "**Solution:** Verify the file was uploaded with `LIST @pdf;`\n",
    "\n",
    "**3. Function Takes Too Long:**\n",
    "- Large PDFs (100+ pages) can take 30-60 seconds\n",
    "- This is normal for the initial processing\n",
    "- Consider processing in batches for very large documents\n",
    "\n",
    "**4. Memory Issues:**\n",
    "- For very large PDFs (500+ pages), you may need to increase warehouse size\n",
    "- Or split the PDF into smaller chunks before processing\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Phase 1: Add Page Numbers & Structured Storage\n",
    "\n",
    "## What We're Adding\n",
    "\n",
    "In Phase 1, we'll enhance the baseline solution with:\n",
    "1. **Page number tracking** - Know which page each text box came from\n",
    "2. **Table storage** - Store results in a queryable table (not VARCHAR)\n",
    "3. **Chunk IDs** - Unique identifiers for each text box\n",
    "4. **Timestamps** - Track when documents were processed\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ Query specific pages: `WHERE page = 5`\n",
    "- ‚úÖ Search across documents: `WHERE text ILIKE '%medication%'`\n",
    "- ‚úÖ Audit trail: When was this document processed?\n",
    "- ‚úÖ Compare multiple PDFs in the same table\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step1"
   },
   "source": [
    "## Step 1: Create Document Chunks Table\n",
    "\n",
    "This table will store the extracted text with metadata:\n",
    "- `chunk_id`: Unique identifier (e.g., 'Prot_000_p5_c42')\n",
    "- `doc_name`: Source PDF filename\n",
    "- `page`: Page number (1-indexed)\n",
    "- `x, y`: Position coordinates\n",
    "- `text`: Extracted text content\n",
    "- `extracted_at`: Timestamp of extraction\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100001"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_create_table",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE document_chunks (\n",
    "    chunk_id VARCHAR PRIMARY KEY,\n",
    "    doc_name VARCHAR NOT NULL,\n",
    "    page INTEGER NOT NULL,\n",
    "    x FLOAT,\n",
    "    y FLOAT,\n",
    "    text VARCHAR,\n",
    "    extracted_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100002"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_verify_table",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Verify table was created\n",
    "DESC TABLE document_chunks;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step2"
   },
   "source": [
    "## Step 2: Enhanced UDF with Page Numbers\n",
    "\n",
    "Now we'll create an **enhanced version** of the UDF that tracks page numbers.\n",
    "\n",
    "### Key Changes:\n",
    "1. `enumerate(pages, start=1)` - Track page numbers starting from 1\n",
    "2. `'page': page_num` - Include page number in output\n",
    "3. Returns JSON with page information\n",
    "\n",
    "### Output Format:\n",
    "```python\n",
    "[{'page': 1, 'pos': (54.0, 720.3), 'txt': 'CLINICAL PROTOCOL'},\n",
    " {'page': 1, 'pos': (72.0, 680.1), 'txt': 'Study Title: ...'},\n",
    " {'page': 2, 'pos': (54.0, 720.3), 'txt': 'Section 1: ...'}]\n",
    "```\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100004"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_enhanced_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION pdf_txt_mapper_v2(scoped_file_url string)\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "ARTIFACT_REPOSITORY = snowflake.snowpark.pypi_shared_repository\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pdfminer')\n",
    "HANDLER = 'main'\n",
    "AS\n",
    "$$\n",
    "import json\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "def main(scoped_file_url):\n",
    "    finding = []\n",
    "    with SnowflakeFile.open(scoped_file_url, 'rb') as f:\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        pages = PDFPage.get_pages(f)\n",
    "        \n",
    "        # Track page numbers with enumerate\n",
    "        for page_num, page in enumerate(pages, start=1):\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            \n",
    "            for lobj in layout:\n",
    "                if isinstance(lobj, LTTextBox):\n",
    "                    x, y, text = lobj.bbox[0], lobj.bbox[3], lobj.get_text()\n",
    "                    # Use list [x, y] instead of tuple (x, y) for valid JSON\n",
    "                    finding.append({\n",
    "                        'page': page_num,\n",
    "                        'pos': [x, y],\n",
    "                        'txt': text\n",
    "                    })\n",
    "    \n",
    "    # Return valid JSON using json.dumps()\n",
    "    return json.dumps(finding)\n",
    "$$;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100005"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_verify_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Verify the enhanced function was created\n",
    "SHOW FUNCTIONS LIKE 'pdf_txt_mapper_v2';\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step3"
   },
   "source": [
    "## Step 3: Test Enhanced UDF\n",
    "\n",
    "Let's test the new UDF to verify it now includes page numbers.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100007"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_test_udf",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Test the enhanced UDF - should now include page numbers\n",
    "SELECT pdf_txt_mapper_v2(build_scoped_file_url(@pdf, 'Prot_000.pdf')) AS extracted_data_with_pages;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step4"
   },
   "source": [
    "## Step 4: Parse and Load Data into Table\n",
    "\n",
    "Now we'll parse the JSON output and load it into our `document_chunks` table.\n",
    "\n",
    "We'll use Snowflake's JSON parsing functions:\n",
    "- `PARSE_JSON()` - Parse the VARCHAR into JSON\n",
    "- `FLATTEN()` - Convert JSON array into rows\n",
    "- `GET()` - Extract specific fields from JSON objects\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100009"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_load_data",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Parse JSON and insert into table\n",
    "INSERT INTO document_chunks (chunk_id, doc_name, page, x, y, text)\n",
    "SELECT \n",
    "    'Prot_000_p' || value:page || '_c' || ROW_NUMBER() OVER (ORDER BY value:page, value:pos[0], value:pos[1]) AS chunk_id,\n",
    "    'Prot_000.pdf' AS doc_name,\n",
    "    value:page::INTEGER AS page,\n",
    "    value:pos[0]::FLOAT AS x,\n",
    "    value:pos[1]::FLOAT AS y,\n",
    "    value:txt::VARCHAR AS text\n",
    "FROM (\n",
    "    SELECT PARSE_JSON(pdf_txt_mapper_v2(build_scoped_file_url(@pdf, 'Prot_000.pdf'))) AS parsed_data\n",
    "),\n",
    "LATERAL FLATTEN(input => parsed_data) AS f;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_step5"
   },
   "source": [
    "## Step 5: Query the Results!\n",
    "\n",
    "Now we can query the extracted data using SQL. This is the **power of Phase 1** - structured, queryable data!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100011"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_count_chunks",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- How many text chunks were extracted?\n",
    "SELECT COUNT(*) AS total_chunks FROM document_chunks;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100012"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_chunks_per_page",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- How many chunks per page?\n",
    "SELECT \n",
    "    page,\n",
    "    COUNT(*) AS chunks_on_page\n",
    "FROM document_chunks\n",
    "GROUP BY page\n",
    "ORDER BY page\n",
    "LIMIT 20;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100013"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_search_medication",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Search for mentions of 'medication' or 'drug'\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    page,\n",
    "    SUBSTR(text, 1, 100) AS text_preview\n",
    "FROM document_chunks\n",
    "WHERE text ILIKE '%medication%'\n",
    "   OR text ILIKE '%drug%'\n",
    "ORDER BY page\n",
    "LIMIT 10;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100014"
  },
  {
   "cell_type": "code",
   "metadata": {
    "name": "phase1_specific_page",
    "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Get all text from a specific page (e.g., page 5)\n",
    "SELECT \n",
    "    chunk_id,\n",
    "    x,\n",
    "    y,\n",
    "    text\n",
    "FROM document_chunks\n",
    "WHERE page = 5\n",
    "ORDER BY y DESC, x;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "phase1_summary"
   },
   "source": [
    "## Phase 1 Summary\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "1. Created `document_chunks` table for structured storage\n",
    "2. Enhanced UDF (`pdf_txt_mapper_v2`) with page number tracking\n",
    "3. Parsed JSON output and loaded into queryable table\n",
    "4. Demonstrated SQL queries on extracted text\n",
    "\n",
    "### üìä New Capabilities:\n",
    "```sql\n",
    "-- Query by page\n",
    "SELECT * FROM document_chunks WHERE page = 5;\n",
    "\n",
    "-- Search for keywords\n",
    "SELECT * FROM document_chunks WHERE text ILIKE '%medication%';\n",
    "\n",
    "-- Count chunks per page\n",
    "SELECT page, COUNT(*) FROM document_chunks GROUP BY page;\n",
    "```\n",
    "\n",
    "### üéØ What This Gives Us:\n",
    "- ‚úÖ **Page numbers** - Know which page every text box came from\n",
    "- ‚úÖ **Queryable data** - Use SQL instead of parsing strings\n",
    "- ‚úÖ **Chunk IDs** - Unique identifiers for traceability\n",
    "- ‚úÖ **Timestamps** - Track when documents were processed\n",
    "- ‚úÖ **Citation foundation** - Can now answer \"This is on page 5\"\n",
    "\n",
    "### ‚ö†Ô∏è Still Missing (Future Phases):\n",
    "- ‚ùå Full bounding boxes (only have x,y corner) ‚Üí Phase 2\n",
    "- ‚ùå Font information (size, bold/italic) ‚Üí Phase 3\n",
    "- ‚ùå Section detection (headers, hierarchy) ‚Üí Phase 4\n",
    "- ‚ùå Smart chunking (semantic boundaries) ‚Üí Phase 5\n",
    "- ‚ùå LLM integration with citations ‚Üí Phase 6\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: Phase 2\n",
    "In Phase 2, we'll capture **full bounding boxes** (x0, y0, x1, y1) instead of just (x, y). This will enable:\n",
    "- Highlighting text in PDF viewers\n",
    "- Detecting multi-column layouts\n",
    "- Calculating text height/width\n",
    "- More accurate positioning for citations\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff100016"
  }
 ]
}
