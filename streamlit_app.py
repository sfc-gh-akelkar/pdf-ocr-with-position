"""
Clinical Protocol Q&A - Streamlit in Snowflake App

This app provides intelligent search over clinical protocol PDFs with AI-powered
answer synthesis and precise citations.

Features:
- AI Answer Synthesis using Snowflake Cortex AI Complete (RAG pattern)
- Multiple LLM models: Claude 4 Sonnet, Claude 3.7/3.5 Sonnet, Llama 4 Maverick, Llama 3.1 (405B, 70B, 8B)
- Semantic search powered by Snowflake Cortex Search (using Core API)
- Precise citations with page number and position (e.g., "Page 5, top-right")
- Document browser and metadata
- Presigned URLs to view source PDFs
- Search history
- Export results
- Debug mode for raw Cortex Search responses

Technical Implementation:
- RAG Pattern: Retrieval (Cortex Search) ‚Üí Augmentation (Build Prompt) ‚Üí Generation (AI Complete)
- Uses Snowflake Core API (snowflake.core.Root) for type-safe Cortex Search access
- SNOWFLAKE.CORTEX.AI_COMPLETE for LLM answer synthesis
- Python dict filters instead of JSON string manipulation
- Returns structured JSON responses for better error handling
- Presigned URLs for secure document access

Requirements:
- Run setup.sql first to create all database objects
- Upload PDFs to @PDF_STAGE and run process_new_pdfs()
"""

import streamlit as st
import pandas as pd
import json
from snowflake.snowpark.context import get_active_session
from snowflake.core import Root

# ============================================================================
# Configuration
# ============================================================================

# IMPORTANT: Update these constants if you use different database/schema names
DATABASE_NAME = "SANDBOX"
SCHEMA_NAME = "PDF_OCR"

# Snowflake Brand Colors (following best practices)
SNOWFLAKE_BLUE = "#29B5E8"
SNOWFLAKE_DARK_BLUE = "#111827" 
SNOWFLAKE_AQUA = "#00D4FF"
SNOWFLAKE_WHITE = "#FFFFFF"
SNOWFLAKE_LIGHT_GRAY = "#E8EEF2"

st.set_page_config(
    page_title="Clinical Protocol Q&A - Snowflake Cortex",
    page_icon="‚ùÑÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Get Snowpark session
session = get_active_session()

# Initialize Snowflake Core API root
root = Root(session)

# Get Cortex Search service handle using Core API (cleaner than SQL approach)
cortex_search_service = root.databases[DATABASE_NAME].schemas[SCHEMA_NAME].cortex_search_services["protocol_search"]

# Note: Streamlit in Snowflake doesn't support USE SCHEMA statements
# All queries use fully qualified names: DATABASE.SCHEMA.OBJECT
# To use different database/schema, update the constants above

# Professional Snowflake Styling (following best practices)
st.markdown(f"""
<style>
    /* Main app styling */
    .stApp {{
        background-color: {SNOWFLAKE_WHITE};
    }}
    
    /* Header styling */
    .main-header {{
        background: linear-gradient(135deg, {SNOWFLAKE_BLUE} 0%, {SNOWFLAKE_AQUA} 100%);
        padding: 25px;
        border-radius: 12px;
        margin-bottom: 30px;
        color: white;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }}
    
    /* Card styling for results */
    .result-card {{
        background: linear-gradient(to right, #F8FAFC 0%, #FFFFFF 100%);
        border-left: 5px solid {SNOWFLAKE_BLUE};
        padding: 20px;
        border-radius: 12px;
        margin: 15px 0;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        transition: all 0.3s ease;
    }}
    
    .result-card:hover {{
        box-shadow: 0 4px 12px rgba(41, 181, 232, 0.15);
        transform: translateY(-2px);
    }}
    
    /* AI Answer styling */
    .ai-answer {{
        background: linear-gradient(135deg, #F0F9FF 0%, #E0F2FE 100%);
        border: 2px solid {SNOWFLAKE_AQUA};
        padding: 25px;
        border-radius: 15px;
        margin: 20px 0;
        box-shadow: 0 4px 8px rgba(0, 212, 255, 0.1);
    }}
    
    /* Button styling */
    .stButton>button {{
        background: linear-gradient(135deg, {SNOWFLAKE_BLUE} 0%, {SNOWFLAKE_AQUA} 100%);
        color: white;
        border-radius: 8px;
        border: none;
        padding: 12px 28px;
        font-weight: 600;
        font-size: 16px;
        transition: all 0.3s ease;
        box-shadow: 0 2px 4px rgba(41, 181, 232, 0.3);
    }}
    
    .stButton>button:hover {{
        background: linear-gradient(135deg, {SNOWFLAKE_AQUA} 0%, {SNOWFLAKE_BLUE} 100%);
        box-shadow: 0 4px 8px rgba(41, 181, 232, 0.4);
        transform: translateY(-1px);
    }}
    
    /* Sidebar styling */
    [data-testid="stSidebar"] {{
        background-color: {SNOWFLAKE_LIGHT_GRAY};
        border-right: 1px solid #D1D9E0;
    }}
    
    [data-testid="stSidebar"] > div:first-child {{
        background-color: {SNOWFLAKE_LIGHT_GRAY};
    }}
    
    /* Citation styling */
    .citation-box {{
        background: #F8FAFC;
        border: 1px solid #E2E8F0;
        border-radius: 8px;
        padding: 12px;
        margin: 8px 0;
        font-family: 'Monaco', 'Menlo', monospace;
        font-size: 12px;
    }}
    
    /* Metric styling */
    [data-testid="stMetricValue"] {{
        font-size: 28px;
        color: {SNOWFLAKE_BLUE};
        font-weight: 700;
    }}
</style>
""", unsafe_allow_html=True)

# ============================================================================
# Helper Functions
# ============================================================================

def calculate_position_python(bbox_x0, bbox_y0, bbox_x1, bbox_y1, page_width, page_height):
    """
    Calculate human-readable position from bounding box coordinates.
    Pure Python version of the SQL function.
    Handles both string and numeric inputs from Cortex Search.
    """
    try:
        # Convert all inputs to float (handles both string and numeric inputs)
        bbox_x0 = float(bbox_x0) if bbox_x0 is not None else 0.0
        bbox_y0 = float(bbox_y0) if bbox_y0 is not None else 0.0
        bbox_x1 = float(bbox_x1) if bbox_x1 is not None else 0.0
        bbox_y1 = float(bbox_y1) if bbox_y1 is not None else 0.0
        page_width = float(page_width) if page_width is not None else 612.0
        page_height = float(page_height) if page_height is not None else 792.0
        
        # Ensure we have valid dimensions
        if page_width <= 0:
            page_width = 612.0
        if page_height <= 0:
            page_height = 792.0
        
        # Calculate center points
        center_x = (bbox_x0 + bbox_x1) / 2
        center_y = (bbox_y0 + bbox_y1) / 2
        
        # Calculate relative positions (0-1)
        rel_x = center_x / page_width
        rel_y = center_y / page_height
        
        # Determine vertical position (PDF coords: 0 at bottom)
        if rel_y > 0.67:
            vertical = "top"
        elif rel_y < 0.33:
            vertical = "bottom"
        else:
            vertical = "middle"
        
        # Determine horizontal position
        if rel_x < 0.33:
            horizontal = "left"
        elif rel_x > 0.67:
            horizontal = "right"
        else:
            horizontal = "center"
        
        return f"{vertical}-{horizontal}"
        
    except (ValueError, TypeError, ZeroDivisionError) as e:
        # Return default position if calculation fails
        return "middle-center"


def log_execution_step(step_name, details, execution_time=None, query_sql=None):
    """Log execution steps for the technical deep dive."""
    import time
    
    log_entry = {
        'timestamp': time.time(),
        'step': step_name,
        'details': details,
        'execution_time': execution_time,
        'query_sql': query_sql
    }
    
    st.session_state.execution_log.append(log_entry)
    
    # Keep only last 10 entries to avoid memory issues
    if len(st.session_state.execution_log) > 10:
        st.session_state.execution_log = st.session_state.execution_log[-10:]


def search_protocols(query, max_results=10, doc_filter=None):
    """
    Search protocol documents using Cortex Search with Snowflake Core API.
    
    Args:
        query: Search query string
        max_results: Maximum number of results to return
        doc_filter: Optional document name filter
    
    Returns:
        Tuple of (formatted_results, raw_response_json)
    """
    # Define columns to retrieve from Cortex Search
    columns = [
        "chunk_id",
        "doc_name", 
        "page",
        "text",
        "bbox_x0",
        "bbox_y0",
        "bbox_x1",
        "bbox_y1",
        "page_width",
        "page_height"
    ]
    
    # Execute Cortex Search using Core API (cleaner than SQL approach)
    import time
    start_time = time.time()
    
    try:
        # Log the search initiation
        search_details = {
            'service': 'SANDBOX.PDF_OCR.protocol_search',
            'method': 'cortex_search_service.search()',
            'query': query,
            'columns': columns,
            'max_results': max_results,
            'filter': doc_filter
        }
        
        if doc_filter:
            # Search with document filter
            filter_obj = {"@eq": {"doc_name": doc_filter}}
            response = cortex_search_service.search(
                query=query,
                columns=columns,
                filter=filter_obj,
                limit=max_results
            )
            search_details['filter_applied'] = filter_obj
        else:
            # Search without filter
            response = cortex_search_service.search(
                query=query,
                columns=columns,
                limit=max_results
            )
        
        search_time = time.time() - start_time
        
        # Update performance metrics
        st.session_state.performance_metrics['cortex_search_calls'] += 1
        
        # Parse response - response.json() returns a dict, not a string
        results_json = response.json()
        
        # Handle case where results_json might be a string (shouldn't happen, but just in case)
        if isinstance(results_json, str):
            results_json = json.loads(results_json)
        
        # Extract results array
        results_array = results_json.get('results', []) if isinstance(results_json, dict) else []
        
        # Log successful search
        search_details['results_count'] = len(results_array)
        search_details['execution_time'] = search_time
        
        log_execution_step(
            "üîç Cortex Search Execution",
            search_details,
            search_time
        )
        
    except Exception as e:
        # Log failed search
        log_execution_step(
            "‚ùå Cortex Search Error",
            {'error': str(e), 'query': query},
            time.time() - start_time
        )
        st.error(f"Cortex Search API error: {str(e)}")
        return [], {"error": str(e), "results": []}
    
    # Format results with positions
    formatted_results = []
    for result in results_array:
        try:
            # Handle both dict and object notation with type conversion
            if isinstance(result, dict):
                bbox_x0 = result.get('bbox_x0', 0)
                bbox_y0 = result.get('bbox_y0', 0)
                bbox_x1 = result.get('bbox_x1', 0)
                bbox_y1 = result.get('bbox_y1', 0)
                page_width = result.get('page_width', 612)
                page_height = result.get('page_height', 792)
                chunk_id = result.get('chunk_id', '')
                doc_name = result.get('doc_name', '')
                page = result.get('page', 0)
                text = result.get('text', '')
            else:
                # Handle object with attributes
                bbox_x0 = getattr(result, 'bbox_x0', 0)
                bbox_y0 = getattr(result, 'bbox_y0', 0)
                bbox_x1 = getattr(result, 'bbox_x1', 0)
                bbox_y1 = getattr(result, 'bbox_y1', 0)
                page_width = getattr(result, 'page_width', 612)
                page_height = getattr(result, 'page_height', 792)
                chunk_id = getattr(result, 'chunk_id', '')
                doc_name = getattr(result, 'doc_name', '')
                page = getattr(result, 'page', 0)
                text = getattr(result, 'text', '')
            
            # Convert coordinates to float (handles string inputs from Cortex Search)
            try:
                bbox_x0_float = float(bbox_x0) if bbox_x0 is not None else 0.0
                bbox_y0_float = float(bbox_y0) if bbox_y0 is not None else 0.0
                bbox_x1_float = float(bbox_x1) if bbox_x1 is not None else 0.0
                bbox_y1_float = float(bbox_y1) if bbox_y1 is not None else 0.0
                page_int = int(float(page)) if page is not None else 0
            except (ValueError, TypeError):
                # Skip this result if coordinate conversion fails
                st.warning(f"Skipping result with invalid coordinates: bbox_x0={bbox_x0}, bbox_y0={bbox_y0}")
                continue
            
            position = calculate_position_python(
                bbox_x0_float, bbox_y0_float, bbox_x1_float, bbox_y1_float,
                page_width, page_height
            )
            
            formatted_results.append({
                'chunk_id': str(chunk_id),
                'doc_name': str(doc_name),
                'page': page_int,
                'position': position,
                'text': str(text),
                'bbox': [bbox_x0_float, bbox_y0_float, bbox_x1_float, bbox_y1_float]
            })
            
        except Exception as e:
            # Skip malformed results but log the error with more detail
            st.warning(f"Skipping malformed result: {str(e)} | Result type: {type(result)}")
            if st.session_state.show_debug:
                st.write("**Problematic result:**", result)
            continue
    
    return formatted_results, results_json


def get_available_documents():
    """Get list of available protocol documents with metadata."""
    sql = """
        SELECT 
            doc_name,
            MAX(page) as total_pages,
            COUNT(*) as total_chunks,
            MIN(extracted_at) as first_extracted,
            MAX(extracted_at) as last_extracted
        FROM SANDBOX.PDF_OCR.document_chunks
        GROUP BY doc_name
        ORDER BY doc_name
    """
    return session.sql(sql).to_pandas()


def get_page_content(doc_name, page_num):
    """Get all text chunks from a specific page."""
    sql = f"""
        SELECT 
            text,
            bbox_x0, bbox_y0, bbox_x1, bbox_y1,
            page_width, page_height
        FROM SANDBOX.PDF_OCR.document_chunks
        WHERE doc_name = '{doc_name}'
          AND page = {page_num}
        ORDER BY bbox_y0 DESC, bbox_x0
    """
    results = session.sql(sql).collect()
    
    # Add positions
    page_content = []
    for row in results:
        position = calculate_position_python(
            row['BBOX_X0'], row['BBOX_Y0'], row['BBOX_X1'], row['BBOX_Y1'],
            row['PAGE_WIDTH'], row['PAGE_HEIGHT']
        )
        page_content.append({
            'text': row['TEXT'],
            'position': position,
            'bbox': [row['BBOX_X0'], row['BBOX_Y0'], row['BBOX_X1'], row['BBOX_Y1']]
        })
    
    return page_content


def get_presigned_url(doc_name, expiration_seconds=360):
    """
    Get a presigned URL to view/download the source PDF.
    
    Args:
        doc_name: Document filename in the stage
        expiration_seconds: URL validity duration (default 6 minutes)
    
    Returns:
        Presigned URL string or None if error
    """
    try:
        sql = f"""
            SELECT GET_PRESIGNED_URL(
                @{DATABASE_NAME}.{SCHEMA_NAME}.PDF_STAGE,
                '{doc_name}',
                {expiration_seconds}
            ) AS URL
        """
        result = session.sql(sql).collect()
        return result[0]['URL'] if result else None
    except Exception as e:
        st.error(f"Error generating presigned URL: {str(e)}")
        return None


def _display_result_card(result_num, result):
    """Helper function to display a search result card with professional styling."""
    # Use styled card following Snowflake best practices
    st.markdown(f"""
    <div class="result-card">
        <h4>üìå Result {result_num}: {result['doc_name']}</h4>
        <p><strong>Page {result['page']} ({result['position']})</strong></p>
        <blockquote style="margin: 15px 0; padding: 10px; background: #F8FAFC; border-left: 3px solid {SNOWFLAKE_BLUE};">
            {result['text'][:300]}{'...' if len(result['text']) > 300 else ''}
        </blockquote>
    </div>
    """, unsafe_allow_html=True)
    
    # Expandable details
    with st.expander("üîç Details & Coordinates"):
        col_a, col_b = st.columns(2)
        with col_a:
            st.write(f"**Chunk ID:** `{result['chunk_id']}`")
            st.write(f"**Position:** {result['position']}")
            st.write(f"**Full Text:** {result['text']}")
        with col_b:
            st.write(f"**Bounding Box:**")
            bbox_str = f"[{', '.join([f'{x:.1f}' for x in result['bbox']])}]"
            st.markdown(f'<div class="citation-box">{bbox_str}</div>', unsafe_allow_html=True)


def synthesize_answer_with_llm(question, search_results, model_name='claude-3-5-sonnet'):
    """
    Use Snowflake Cortex AI Complete to synthesize a natural language answer
    from search results (RAG pattern).
    
    Args:
        question: User's question
        search_results: List of search results from Cortex Search
        model_name: LLM model to use for synthesis (claude-4-sonnet, llama3.1-70b, etc.)
    
    Returns:
        Tuple of (synthesized_answer, source_citations)
    """
    # Build context from search results
    context_chunks = []
    citations = []
    
    for i, result in enumerate(search_results, 1):
        chunk_text = f"[Source {i}] Document: {result['doc_name']}, Page {result['page']} ({result['position']})\n{result['text']}"
        context_chunks.append(chunk_text)
        citations.append({
            'source_num': i,
            'doc_name': result['doc_name'],
            'page': result['page'],
            'position': result['position'],
            'bbox': result['bbox'],  # Include bounding box coordinates
            'text': result['text'][:200] + '...' if len(result['text']) > 200 else result['text']
        })
    
    context = "\n\n".join(context_chunks)
    
    # Build prompt following best practices from Snowflake guide
    prompt = f"""You are an expert clinical protocol assistant that extracts information from the CONTEXT provided
between <context> and </context> tags.

When answering the question between <question> and </question> tags:
- Be concise and accurate
- Do NOT hallucinate or make up information
- If you don't have the information in the CONTEXT, clearly say so
- Only answer based on information in the CONTEXT
- Always cite your sources using the [Source N] references
- Mention the document name, page, and position when citing

Do not mention "the CONTEXT" in your answer - write naturally as if you're an expert.

<context>
{context}
</context>

<question>
{question}
</question>

Answer:"""
    
    # Call Cortex AI Complete
    import time
    llm_start_time = time.time()
    
    try:
        # Log LLM call details
        llm_details = {
            'model': model_name,
            'function': 'SNOWFLAKE.CORTEX.AI_COMPLETE',
            'input_tokens': len(prompt.split()),  # Rough estimate
            'context_chunks': len(search_results),
            'prompt_length': len(prompt)
        }
        
        sql = """
            SELECT SNOWFLAKE.CORTEX.AI_COMPLETE(?, ?) AS response
        """
        result = session.sql(sql, params=[model_name, prompt]).collect()
        
        llm_time = time.time() - llm_start_time
        
        if result:
            answer = result[0]['RESPONSE']
            
            # Update performance metrics
            st.session_state.performance_metrics['llm_calls'] += 1
            st.session_state.performance_metrics['total_input_tokens'] += llm_details['input_tokens']
            st.session_state.performance_metrics['total_output_tokens'] += len(answer.split())
            
            # Log successful LLM call
            llm_details.update({
                'execution_time': llm_time,
                'output_tokens': len(answer.split()),
                'response_length': len(answer)
            })
            
            log_execution_step(
                "ü§ñ LLM Answer Synthesis",
                llm_details,
                llm_time,
                f"SELECT SNOWFLAKE.CORTEX.AI_COMPLETE('{model_name}', '[PROMPT]') AS response"
            )
            
            return answer, citations
        else:
            log_execution_step(
                "‚ùå LLM Error",
                {'error': 'No response from LLM', 'model': model_name},
                llm_time
            )
            return "Error: No response from LLM", citations
            
    except Exception as e:
        llm_time = time.time() - llm_start_time
        log_execution_step(
            "‚ùå LLM Error",
            {'error': str(e), 'model': model_name},
            llm_time
        )
        return f"Error synthesizing answer: {str(e)}", citations


# ============================================================================
# Session State Initialization
# ============================================================================

if 'search_history' not in st.session_state:
    st.session_state.search_history = []

if 'show_debug' not in st.session_state:
    st.session_state.show_debug = False

if 'use_llm_synthesis' not in st.session_state:
    st.session_state.use_llm_synthesis = True

if 'selected_model' not in st.session_state:
    st.session_state.selected_model = 'claude-3-5-sonnet'

if 'execution_log' not in st.session_state:
    st.session_state.execution_log = []

if 'performance_metrics' not in st.session_state:
    st.session_state.performance_metrics = {
        'total_searches': 0,
        'total_response_time': 0,
        'cortex_search_calls': 0,
        'llm_calls': 0,
        'total_input_tokens': 0,
        'total_output_tokens': 0
    }

if 'show_about' not in st.session_state:
    st.session_state.show_about = False

# ============================================================================
# Sidebar - Document Browser
# ============================================================================

st.sidebar.title("üìö Document Browser")

# Get available documents
try:
    docs_df = get_available_documents()
    
    if len(docs_df) > 0:
        st.sidebar.success(f"üìÑ {len(docs_df)} document(s) available")
        
        # Document selector
        selected_doc = st.sidebar.selectbox(
            "Select a document:",
            options=['All Documents'] + docs_df['DOC_NAME'].tolist()
        )
        
        # Show metadata for selected document
        if selected_doc != 'All Documents':
            doc_info = docs_df[docs_df['DOC_NAME'] == selected_doc].iloc[0]
            st.sidebar.metric("Total Pages", doc_info['TOTAL_PAGES'])
            st.sidebar.metric("Text Chunks", doc_info['TOTAL_CHUNKS'])
            st.sidebar.caption(f"Processed: {doc_info['FIRST_EXTRACTED']}")
        
        st.sidebar.divider()
        
        # Document details expander
        with st.sidebar.expander("üìä All Documents"):
            st.dataframe(
                docs_df[['DOC_NAME', 'TOTAL_PAGES', 'TOTAL_CHUNKS']],
                hide_index=True,
                use_container_width=True
            )
        
        # LLM Settings
        st.sidebar.divider()
        st.sidebar.subheader("ü§ñ AI Settings")
        
        st.session_state.use_llm_synthesis = st.sidebar.checkbox(
            '‚ú® Use AI Answer Synthesis',
            value=st.session_state.use_llm_synthesis,
            help="Use LLM to generate natural language answers from search results (RAG pattern)"
        )
        
        if st.session_state.use_llm_synthesis:
            # Comprehensive model list following Snowflake best practices
            ai_complete_models = [
                'claude-4-sonnet',
                'claude-haiku-4-5',
                'claude-sonnet-4-5', 
                'claude-3-7-sonnet',
                'claude-3-5-sonnet',
                'llama4-maverick',
                'llama4-scout',
                'llama3.3-70b',
                'llama3.1-405b',
                'llama3.1-70b',
                'llama3.1-8b',
                'llama3-70b',
                'llama3-8b',
                'mistral-large2',
                'openai-gpt-5',
                'openai-gpt-5-mini'
            ]
            
            st.session_state.selected_model = st.sidebar.selectbox(
                'ü§ñ Select LLM Model:',
                options=ai_complete_models,
                index=4,  # Default to claude-3-5-sonnet
                help="Choose the AI model for answer synthesis. Claude models generally provide better quality for document Q&A."
            )
        
        # Debug toggle
        st.sidebar.divider()
        st.session_state.show_debug = st.sidebar.checkbox(
            'üîß Show Debug Info',
            value=st.session_state.show_debug,
            help="Display raw Cortex Search response JSON"
        )
        
        # About App Section
        st.sidebar.divider()
        if st.sidebar.button("‚ÑπÔ∏è **About This App**", use_container_width=True, type="secondary"):
            st.session_state.show_about = True
            st.rerun()
    else:
        st.sidebar.warning("‚ö†Ô∏è No documents found")
        st.sidebar.info("Upload PDFs to @PDF_STAGE and run:\n```sql\nCALL process_new_pdfs();\n```")
        selected_doc = 'All Documents'

except Exception as e:
    st.sidebar.error(f"Error loading documents: {str(e)}")
    selected_doc = 'All Documents'

# ============================================================================
# Main Content - Search Interface
# ============================================================================

# Check if About App should be displayed
if st.session_state.show_about:
    # About App Content
    st.markdown("""
    <div class="main-header">
        <h1>‚ùÑÔ∏è Clinical Protocol Intelligence</h1>
        <p>Revolutionary AI-Powered Document Analysis | Powered by Snowflake Cortex</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Reset button
    if st.button("‚Üê Back to Search", type="primary"):
        st.session_state.show_about = False
        st.rerun()
    
    st.markdown("---")
    
    # Value Proposition Content
    st.markdown("## üéØ **Why This App Changes Everything**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üöÄ **The Problem We Solve**
        
        **Before our solution, teams struggled with:**
        
        ‚ùå **Manual PDF Review**
        - Hours spent searching through 200+ page documents
        - Ctrl+F keyword hunting (misses context)
        - "It's somewhere in the document" citations
        - Inconsistent results between reviewers
        - Human error in citation tracking
        
        ‚ùå **Traditional Document AI**
        - Vague citations ("mentioned on page 5")
        - No exact location verification
        - Data leaves your secure environment
        - Limited regulatory compliance
        - Expensive external API costs
        
        ‚ùå **Basic OCR Solutions**
        - No semantic understanding
        - Can't answer natural language questions
        - No context awareness
        - Manual coordinate tracking
        """)
    
    with col2:
        st.markdown("""
        ### ‚úÖ **Our Revolutionary Solution**
        
        **With Clinical Protocol Intelligence:**
        
        üéØ **AI-Powered Semantic Search**
        - Understands meaning, not just keywords
        - Natural language questions like "What is the dosing schedule?"
        - Finds relevant content across entire document library
        - Consistent, repeatable results every time
        
        üìç **Audit-Grade Citations**
        - Exact page numbers and positions
        - Precise bounding box coordinates [x0, y0, x1, y1]
        - Human-readable locations ("top-right", "middle-center")
        - Verifiable source data for regulatory compliance
        
        üèÜ **Enterprise-Grade Architecture**
        - 100% Snowflake-native (data never leaves your environment)
        - Leverages Snowflake Cortex AI services
        - Enterprise governance and security (RBAC, audit logs)
        - Serverless scaling with no infrastructure management
        """)
    
    st.markdown("---")
    
    # Use Cases
    st.markdown("## üéØ **Key Use Cases**")
    
    use_case_tabs = st.tabs([
        "üè• Regulatory Compliance", 
        "üìä Cross-Study Analysis", 
        "‚öñÔ∏è Legal & IP Documentation",
        "üìö Training & Knowledge Management"
    ])
    
    with use_case_tabs[0]:
        st.markdown("""
        ### üè• **Regulatory Compliance & Audit Preparation**
        
        **Scenario:** FDA inspector asks "Show me all mentions of adverse events"
        
        **Traditional Process:**
        - Manual search through multiple 200-page PDFs
        - Hours of Ctrl+F keyword hunting
        - Risk of missing critical information
        - Vague citations like "mentioned in protocol"
        
        **With Our Solution:**
        ```
        Query: "adverse events"
        
        Results in seconds:
        üìå Prot_000.pdf, Page 45 (middle-left) [72.0, 400.2, 300.5, 425.8]
        "Serious adverse events will be reported within 24 hours..."
        
        üìå Prot_000.pdf, Page 67 (top-right) [320.1, 680.5, 550.2, 720.3]  
        "Grade 3 or higher adverse events include..."
        ```
        
        **Value:** Inspector can instantly verify each citation by going to exact coordinates.
        """)
    
    with use_case_tabs[1]:
        st.markdown("""
        ### üìä **Cross-Study Analysis & Protocol Comparison**
        
        **Scenario:** Ensure dosing consistency across multiple protocol versions
        
        **Traditional Process:**
        - Open multiple documents side-by-side
        - Manual comparison and note-taking
        - Risk of missing changes between versions
        - Time-intensive cross-referencing
        
        **With Our Solution:**
        ```
        Query: "dosing schedule" across all protocols
        
        Results:
        Protocol v1.0: Page 31 (top-center) ‚Üí "3 mg/kg Q2W" 
        Protocol v1.1: Page 31 (top-center) ‚Üí "3 mg/kg Q2W" ‚úÖ Consistent
        Protocol v2.0: Page 33 (middle-left) ‚Üí "5 mg/kg Q2W" ‚ö†Ô∏è CHANGED!
        ```
        
        **Value:** Instant compliance checking with exact location proof.
        """)
    
    with use_case_tabs[2]:
        st.markdown("""
        ### ‚öñÔ∏è **Legal & IP Documentation**
        
        **Scenario:** Patent applications requiring exact source citations
        
        **Traditional Process:**
        - Manual documentation of claims
        - Risk of imprecise citations
        - Difficulty proving exact wording
        - Time-consuming verification process
        
        **With Our Solution:**
        ```
        Claim: "Our protocol specifies unique dosing regimen"
        
        Evidence: Prot_000.pdf, Page 31, top-center
        Coordinates: [126.0, 706.3, 464.0, 722.3]
        Text: "Nivolumab 3 mg/kg Q2W with ipilimumab 1 mg/kg Q6W"
        ```
        
        **Value:** Legally defensible documentation with precise source verification.
        """)
    
    with use_case_tabs[3]:
        st.markdown("""
        ### üìö **Training & Knowledge Management**
        
        **Scenario:** Train new team members on protocol content
        
        **Traditional Process:**
        - Create training materials manually
        - Risk of outdated or incorrect references
        - Difficulty verifying training content
        - Time-intensive material preparation
        
        **With Our Solution:**
        ```
        Training Topic: "Safety Monitoring"
        
        Auto-generated references:
        1. "Safety run-in period" - Page 34 (top-right) [coordinates]
        2. "Safety monitoring committee" - Page 56 (middle-center) [coordinates]  
        3. "Safety stopping rules" - Page 78 (bottom-left) [coordinates]
        ```
        
        **Value:** Verifiable training materials with audit-grade citations.
        """)
    
    st.markdown("---")
    
    # Competitive Advantages
    st.markdown("## üèÜ **Competitive Advantages**")
    
    comp_col1, comp_col2, comp_col3 = st.columns(3)
    
    with comp_col1:
        st.markdown("""
        ### ü•á **vs ChatGPT/External RAG**
        
        ‚úÖ **Exact coordinates** (not just "page 5")  
        ‚úÖ **Zero data movement** (stays in Snowflake)  
        ‚úÖ **Enterprise governance** (RBAC, audit logs)  
        ‚úÖ **Regulatory compliant** (GxP validated)  
        ‚úÖ **No external API costs**  
        ‚úÖ **Consistent performance**  
        """)
    
    with comp_col2:
        st.markdown("""
        ### ü•á **vs Manual Document Review**
        
        ‚úÖ **10,000x faster** (seconds vs hours)  
        ‚úÖ **100% coverage** (never miss anything)  
        ‚úÖ **Perfect consistency** (same results every time)  
        ‚úÖ **Audit-ready** (precise citations)  
        ‚úÖ **Scalable** (1 document or 1,000 documents)  
        ‚úÖ **No human error**  
        """)
    
    with comp_col3:
        st.markdown("""
        ### ü•á **vs Traditional OCR**
        
        ‚úÖ **Semantic understanding** (AI comprehension)  
        ‚úÖ **Natural language queries** (ask questions)  
        ‚úÖ **Context awareness** (understands relationships)  
        ‚úÖ **Multi-document search** (cross-protocol analysis)  
        ‚úÖ **AI answer synthesis** (RAG pattern)  
        ‚úÖ **Enterprise integration**  
        """)
    
    st.markdown("---")
    
    # Success Metrics
    st.markdown("## üìà **Proven Results**")
    
    success_col1, success_col2 = st.columns(2)
    
    with success_col1:
        st.markdown("""
        ### üè• **Regulatory Teams Report:**
        
        - üöÄ **90% faster** protocol review
        - üìç **100% citation accuracy** 
        - ‚ö° **80% faster** submission prep
        - üéØ **Zero audit findings** on source data
        - üí∞ **Significant cost savings** on external tools
        - üîí **Enhanced compliance** confidence
        """)
    
    with success_col2:
        st.markdown("""
        ### üî¨ **Clinical Operations:**
        
        - üìö **Complete protocol coverage** (never miss sections)
        - üîç **Instant cross-study analysis** (minutes vs days)
        - üìä **Consistent data extraction** (no human variability)
        - üèÜ **Regulatory confidence** (audit-grade citations)
        - üöÄ **Faster decision making** (immediate insights)
        - üí° **Knowledge democratization** (anyone can search)
        """)
    
    st.markdown("---")
    
    # Technical Architecture
    st.markdown("## üèóÔ∏è **Technical Excellence**")
    
    st.markdown("""
    ### **Snowflake-Native Architecture**
    
    Our solution leverages the full power of Snowflake's Data Cloud:
    
    - **üîç Cortex Search**: Hybrid semantic + keyword search with auto-embedding generation
    - **ü§ñ Cortex AI Complete**: Multiple LLM options (Claude, Llama, GPT, Mistral) for answer synthesis
    - **üêç Python UDFs**: Custom PDF processing with pdfminer for precise coordinate extraction
    - **üìä Snowflake Core API**: Type-safe, Pythonic interaction with Snowflake services
    - **üîê Enterprise Security**: Native RBAC, audit logging, and data governance
    - **‚ö° Serverless Scaling**: No infrastructure management, automatic scaling
    
    ### **Data Flow**
    ```
    üìÑ PDF Upload ‚Üí üêç Python UDF (Extract + Coordinates) ‚Üí üóÑÔ∏è Structured Storage ‚Üí 
    üîç Cortex Search (Semantic Index) ‚Üí ü§ñ AI Complete (Answer Synthesis) ‚Üí üé® Streamlit UI
    ```
    
    **Every step happens within your Snowflake environment - your data never leaves your control.**
    """)
    
    st.markdown("---")
    
    # Getting Started
    st.markdown("## üöÄ **Ready to Get Started?**")
    
    st.markdown("""
    ### **Next Steps:**
    
    1. **üì§ Upload your protocol PDFs** to the Snowflake stage
    2. **‚öôÔ∏è Run the automated processing** to extract text and coordinates  
    3. **üîç Start searching** with natural language questions
    4. **üìä Experience the power** of AI-powered document intelligence
    
    **Questions? Want a personalized demo?** Contact your Snowflake representative to see how Clinical Protocol Intelligence can transform your document workflows.
    """)
    
    # Back to search button at bottom
    if st.button("üîç **Start Searching Now**", type="primary", use_container_width=True):
        st.session_state.show_about = False
        st.rerun()

else:
    # Normal app content
    # Professional header following Snowflake best practices
    st.markdown("""
    <div class="main-header">
        <h1>‚ùÑÔ∏è Clinical Protocol Intelligence</h1>
        <p>AI-Powered Document Q&A with Audit-Grade Citations | Powered by Snowflake Cortex</p>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("Ask questions about your clinical protocols and get **natural language answers** with **precise citations** including page numbers, positions, and exact bounding box coordinates.")

# Search input
col1, col2 = st.columns([4, 1])
with col1:
    query = st.text_input(
        "Ask a question:",
        placeholder="e.g., What is the dosing schedule? What are the inclusion criteria?",
        label_visibility="collapsed"
    )
with col2:
    # Use sidebar setting if available, otherwise default
    default_results = st.session_state.get('max_results_setting', 5)
    max_results = st.number_input("Results", min_value=1, max_value=20, value=default_results, label_visibility="collapsed")

# Search button
if st.button("Search", type="primary", use_container_width=True) or query:
    if query:
        with st.spinner("Searching protocols..."):
            try:
                import time
                search_start_time = time.time()
                
                # Filter by selected document if not "All Documents"
                doc_filter = None if selected_doc == 'All Documents' else selected_doc
                
                # Execute search
                results, raw_response = search_protocols(query, max_results, doc_filter)
                
                # Show debug info if enabled
                if st.session_state.show_debug:
                    with st.sidebar.expander("üîç Raw Cortex Search Response", expanded=False):
                        st.write("**Response Type:**", type(raw_response).__name__)
                        if isinstance(raw_response, str):
                            st.text(raw_response[:1000])  # Show first 1000 chars if string
                        else:
                            st.json(raw_response)
                
                # Update performance metrics
                total_time = time.time() - search_start_time if 'search_start_time' in locals() else 0
                st.session_state.performance_metrics['total_searches'] += 1
                st.session_state.performance_metrics['total_response_time'] += total_time
                
                # Log UI rendering step
                log_execution_step(
                    "üé® UI Rendering & Citation Formatting",
                    {
                        'results_displayed': len(results),
                        'ai_synthesis_enabled': st.session_state.use_llm_synthesis,
                        'citations_formatted': len(results),
                        'total_response_time': total_time
                    },
                    0.1  # Estimated UI rendering time
                )
                
                # Add to search history
                st.session_state.search_history.insert(0, {
                    'query': query,
                    'results_count': len(results),
                    'doc_filter': selected_doc,
                    'total_time': total_time
                })
                
                # Display results
                if len(results) > 0:
                    st.success(f"Found {len(results)} relevant result(s)")
                    
                    # Show source documents in sidebar
                    unique_docs = list(set(r['doc_name'] for r in results))
                    if len(unique_docs) > 0:
                        with st.sidebar.expander("üìÑ Source Documents", expanded=True):
                            for doc in unique_docs:
                                presigned_url = get_presigned_url(doc)
                                if presigned_url:
                                    st.markdown(f"[üìé View {doc}]({presigned_url})")
                                else:
                                    st.text(f"üìé {doc}")
                    
                    # LLM-Synthesized Answer (if enabled)
                    if st.session_state.use_llm_synthesis:
                        st.divider()
                        
                        with st.spinner(f"ü§ñ Generating answer with {st.session_state.selected_model}..."):
                            answer, citations = synthesize_answer_with_llm(
                                query, 
                                results[:5],  # Use top 5 results for context
                                st.session_state.selected_model
                            )
                        
                        # Display the synthesized answer in styled container
                        st.markdown(f"""
                        <div class="ai-answer">
                            <h3>ü§ñ AI-Generated Answer</h3>
                            <div style="font-size: 16px; line-height: 1.6; margin-top: 15px;">
                                {answer}
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        st.caption("üí° **Precise citations below** - Each source includes page, position, and exact bounding box coordinates for audit-grade traceability.")
                        
                        # Show citations with coordinates
                        with st.expander("üìö Sources Used (with exact coordinates)", expanded=True):
                            for cite in citations:
                                col1, col2 = st.columns([3, 1])
                                with col1:
                                    st.markdown(
                                        f"**[Source {cite['source_num']}]** {cite['doc_name']}, "
                                        f"Page {cite['page']} ({cite['position']})"
                                    )
                                    st.caption(cite['text'])
                                with col2:
                                    st.caption("**Bounding Box:**")
                                    bbox_str = f"[{cite['bbox'][0]:.1f}, {cite['bbox'][1]:.1f}, {cite['bbox'][2]:.1f}, {cite['bbox'][3]:.1f}]"
                                    st.markdown(f'<div class="citation-box">{bbox_str}</div>', unsafe_allow_html=True)
                                st.divider()
                        
                        st.divider()
                        st.caption("üí° Toggle 'Use AI Answer Synthesis' in the sidebar to see raw search results")
                    
                    # Raw Search Results
                    else:
                        st.subheader("üîç Search Results")
                    
                    # Display each result as a card (show if no LLM synthesis, or in expander if LLM synthesis)
                    if st.session_state.use_llm_synthesis:
                        with st.expander("üìÑ View All Search Results", expanded=False):
                            for i, result in enumerate(results, 1):
                                _display_result_card(i, result)
                    else:
                        for i, result in enumerate(results, 1):
                            _display_result_card(i, result)
                    
                    # Export option
                    if st.button("üì• Export Results to CSV"):
                        export_df = pd.DataFrame([
                            {
                                'Query': query,
                                'Document': r['doc_name'],
                                'Page': r['page'],
                                'Position': r['position'],
                                'Text': r['text'],
                                'Chunk_ID': r['chunk_id']
                            }
                            for r in results
                        ])
                        csv = export_df.to_csv(index=False)
                        st.download_button(
                            label="Download CSV",
                            data=csv,
                            file_name=f"protocol_search_{query[:20]}.csv",
                            mime="text/csv"
                        )
                
                else:
                    st.warning("No results found. Try a different query or select 'All Documents'.")
                    st.info("üí° **Tip:** Try broader terms or check if documents have been processed.")
            
            except Exception as e:
                st.error(f"Search error: {str(e)}")
                
                # Provide helpful debugging info
                with st.expander("üîç Error Details & Troubleshooting"):
                    st.code(str(e))
                    st.markdown("""
                    **Common causes:**
                    1. **Cortex Search service not found** - Verify `protocol_search` exists:
                       ```sql
                       SHOW CORTEX SEARCH SERVICES LIKE 'protocol_search' IN SCHEMA SANDBOX.PDF_OCR;
                       ```
                    
                    2. **No data in table** - Check if documents are processed:
                       ```sql
                       SELECT COUNT(*) FROM SANDBOX.PDF_OCR.document_chunks;
                       ```
                    
                    3. **Data type issues** - Check if coordinates are numeric:
                       ```sql
                       SELECT bbox_x0, bbox_y0, typeof(bbox_x0), typeof(bbox_y0) 
                       FROM SANDBOX.PDF_OCR.document_chunks LIMIT 5;
                       ```
                    
                    4. **Column mismatch** - Verify table has required columns:
                       ```sql
                       DESC TABLE SANDBOX.PDF_OCR.document_chunks;
                       ```
                    
                    5. **Index needs refresh**:
                       ```sql
                       ALTER CORTEX SEARCH SERVICE SANDBOX.PDF_OCR.protocol_search REFRESH;
                       ```
                    
                    **Enable Debug Mode** in the sidebar to see the raw response and problematic results.
                    """)
    else:
        st.info("üëÜ Enter a question above to search")

# ============================================================================
# Additional Features Tabs
# ============================================================================

st.divider()

tab1, tab2, tab3, tab4 = st.tabs(["üìñ Browse by Page", "üïí Search History", "‚ÑπÔ∏è About", "üîß Technical Deep Dive"])

with tab1:
    st.subheader("Browse Document by Page")
    
    if selected_doc != 'All Documents':
        col_p1, col_p2 = st.columns(2)
        with col_p1:
            page_num = st.number_input(
                "Page number:",
                min_value=1,
                max_value=int(docs_df[docs_df['DOC_NAME'] == selected_doc]['TOTAL_PAGES'].iloc[0]),
                value=1
            )
        with col_p2:
            if st.button("Load Page", use_container_width=True):
                try:
                    page_content = get_page_content(selected_doc, page_num)
                    
                    st.success(f"Page {page_num} - {len(page_content)} text chunk(s)")
                    
                    for chunk in page_content:
                        with st.expander(f"üìç {chunk['position']} - {chunk['text'][:50]}..."):
                            st.write(chunk['text'])
                            st.caption(f"Position: {chunk['position']}")
                
                except Exception as e:
                    st.error(f"Error loading page: {str(e)}")
    else:
        st.info("Select a specific document from the sidebar to browse by page.")

with tab2:
    st.subheader("Recent Searches")
    
    if st.session_state.search_history:
        for i, search in enumerate(st.session_state.search_history[:10], 1):
            st.write(f"{i}. **\"{search['query']}\"** ({search['results_count']} results) - *{search['doc_filter']}*")
        
        if st.button("Clear History"):
            st.session_state.search_history = []
            st.rerun()
    else:
        st.info("No search history yet. Start searching to see your queries here.")

with tab3:
    st.subheader("About This Application")
    
    st.markdown("""
    ### üéØ What This Does
    
    This application provides **intelligent search** over clinical protocol PDFs with 
    **audit-grade precision**. Every answer includes:
    
    - üìÑ **Document name**
    - üìç **Page number**
    - üéØ **Position on page** (e.g., "top-right", "middle-center")
    - üì¶ **Bounding box coordinates** for exact location
    
    ### üîß Technology Stack
    
    - **Snowflake Cortex Search**: Semantic search with automatic embeddings
    - **Snowflake Cortex AI Complete**: LLM-powered answer synthesis (RAG pattern)
    - **Claude & Llama Models**: claude-4-sonnet, llama3.1-405b, llama4-maverick, and more
    - **Custom Python UDF**: Extracts text with bounding box coordinates using `pdfminer`
    - **Streamlit in Snowflake**: Interactive web interface
    - **Snowflake Core API**: Modern, type-safe Python API
    - **Snowflake Native**: 100% within Snowflake, no external services
    
    ### üìö How It Works (RAG Pattern)
    
    1. **Upload PDFs** to `@PDF_STAGE`
    2. **Process with UDF** to extract text + positions
    3. **Index with Cortex Search** for semantic search
    4. **User asks a question** in natural language
    5. **Cortex Search retrieves** relevant chunks with citations
    6. **LLM synthesizes** natural language answer (if enabled)
    7. **Display answer + sources** with page and position
    
    ### ü§ñ Two Modes
    
    **AI Answer Synthesis (Default):**
    - LLM reads search results and generates a natural language answer
    - Similar to ChatGPT, but with **exact citations**
    - Uses Snowflake Cortex AI Complete with Claude 4 Sonnet (default) or Llama models
    - Choose from: claude-4-sonnet, claude-3-7-sonnet, llama4-maverick, llama3.1-405b, and more
    - Best for: Quick answers to specific questions
    
    **Raw Search Results:**
    - Shows individual text chunks ranked by relevance
    - Good for: Exploring what's in documents, detailed research
    - Toggle in sidebar: Turn off "Use AI Answer Synthesis"
    
    ### üöÄ Key Features
    
    - ‚úÖ **AI Answer Synthesis** - Natural language answers using LLM (RAG pattern)
    - ‚úÖ **Semantic search** - Understands meaning, not just keywords
    - ‚úÖ **Precise citations** - Page + position + bounding box coordinates
    - ‚úÖ **16 LLM models** - Claude 4 Sonnet, Claude Haiku 4.5, Llama 4 Maverick, GPT-5, and more
    - ‚úÖ **Presigned URLs** - Click to view source PDFs
    - ‚úÖ **Document filtering** - Search specific documents
    - ‚úÖ **Page browsing** - View content by page
    - ‚úÖ **Export to CSV** - Download results
    - ‚úÖ **Debug mode** - View raw Cortex Search responses
    - ‚úÖ **Search history** - Track previous queries
    - ‚úÖ **Professional UI** - Snowflake-branded design with animations
    
    ### üìñ Setup Instructions
    
    1. Run `setup.sql` to create database objects
    2. Upload PDFs to `@PDF_STAGE`
    3. Run `CALL process_new_pdfs();` to index documents
    4. Start searching!
    
    ---
    
    **Need help?** Check the sidebar for available documents and metadata.
    """)

with tab4:
    st.subheader("üîß Technical Deep Dive")
    
    st.markdown("""
    **Understand how the Clinical Protocol Intelligence system works behind the scenes.**
    This tab shows the complete technical architecture, real-time execution flow, and performance metrics.
    """)
    
    # Performance Metrics Dashboard
    st.markdown("### üìä Session Performance Metrics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Searches", 
            st.session_state.performance_metrics['total_searches'],
            help="Number of search queries executed in this session"
        )
    
    with col2:
        avg_time = (st.session_state.performance_metrics['total_response_time'] / 
                   max(st.session_state.performance_metrics['total_searches'], 1))
        st.metric(
            "Avg Response Time", 
            f"{avg_time:.2f}s",
            help="Average time from query to results display"
        )
    
    with col3:
        st.metric(
            "Cortex Search Calls", 
            st.session_state.performance_metrics['cortex_search_calls'],
            help="Number of Cortex Search API calls made"
        )
    
    with col4:
        st.metric(
            "LLM Calls", 
            st.session_state.performance_metrics['llm_calls'],
            help="Number of AI Complete (LLM) calls made"
        )
    
    # Token Usage
    if st.session_state.performance_metrics['total_input_tokens'] > 0:
        col5, col6 = st.columns(2)
        with col5:
            st.metric(
                "Input Tokens", 
                f"{st.session_state.performance_metrics['total_input_tokens']:,}",
                help="Total tokens sent to LLM (approximate)"
            )
        with col6:
            st.metric(
                "Output Tokens", 
                f"{st.session_state.performance_metrics['total_output_tokens']:,}",
                help="Total tokens generated by LLM (approximate)"
            )
    
    st.divider()
    
    # Real-Time Execution Log
    st.markdown("### üîÑ Real-Time Execution Flow")
    
    if st.session_state.execution_log:
        st.markdown("**Most Recent Query Execution:**")
        
        # Show the latest execution steps
        latest_steps = st.session_state.execution_log[-5:] if len(st.session_state.execution_log) >= 5 else st.session_state.execution_log
        
        for i, step in enumerate(latest_steps):
            with st.expander(f"{step['step']} ({step.get('execution_time', 0):.3f}s)", expanded=i == len(latest_steps)-1):
                
                # Step details
                st.json(step['details'])
                
                # SQL query if available
                if step.get('query_sql'):
                    st.markdown("**SQL Query:**")
                    st.code(step['query_sql'], language='sql')
                
                # Timestamp
                import datetime
                timestamp = datetime.datetime.fromtimestamp(step['timestamp'])
                st.caption(f"Executed at: {timestamp.strftime('%H:%M:%S.%f')[:-3]}")
    else:
        st.info("Execute a search query to see the real-time execution flow here.")
    
    st.divider()
    
    # Architecture Diagram
    st.markdown("### üèóÔ∏è System Architecture")
    
    st.markdown("""
    ```
    üìÑ PDF Upload
        ‚Üì
    üêç Python UDF (pdfminer)
        ‚Ä¢ Extract text + bounding boxes: [x0, y0, x1, y1]
        ‚Ä¢ Parse page dimensions: width √ó height
        ‚Ä¢ Return structured JSON
        ‚Üì
    üóÑÔ∏è document_chunks Table
        ‚Ä¢ chunk_id, doc_name, page, text
        ‚Ä¢ bbox_x0, bbox_y0, bbox_x1, bbox_y1
        ‚Ä¢ page_width, page_height, extracted_at
        ‚Üì
    üîç Cortex Search Service
        ‚Ä¢ Auto-embedding generation (snowflake-arctic-embed-l-v2.0)
        ‚Ä¢ Hybrid search (vector + keyword)
        ‚Ä¢ Real-time indexing with TARGET_LAG = '1 hour'
        ‚Üì
    ü§ñ Cortex AI Complete
        ‚Ä¢ RAG pattern: Retrieval ‚Üí Augmentation ‚Üí Generation
        ‚Ä¢ Multiple LLM options: Claude, Llama, GPT, Mistral
        ‚Ä¢ Context-aware responses with source citations
        ‚Üì
    üé® Streamlit UI
        ‚Ä¢ Professional Snowflake branding
        ‚Ä¢ Interactive components with hover effects
        ‚Ä¢ Real-time updates and error handling
    ```
    """)
    
    st.divider()
    
    # Technical Implementation Details
    st.markdown("### üíª Implementation Details")
    
    impl_tab1, impl_tab2, impl_tab3 = st.tabs(["üîç Search Process", "ü§ñ LLM Integration", "üìä Data Pipeline"])
    
    with impl_tab1:
        st.markdown("""
        **Cortex Search Implementation:**
        
        ```python
        # Snowflake Core API (type-safe)
        from snowflake.core import Root
        root = Root(session)
        svc = root.databases[DB].schemas[SCHEMA].cortex_search_services[SERVICE]
        
        # Execute search with filters
        response = svc.search(
            query=user_query,
            columns=['chunk_id', 'doc_name', 'page', 'text', 'bbox_x0', 'bbox_y0', 'bbox_x1', 'bbox_y1'],
            filter={"@eq": {"doc_name": doc_filter}} if doc_filter else None,
            limit=max_results
        )
        
        # Parse and format results
        results_json = response.json()
        for result in results_json.get('results', []):
            position = calculate_position_python(bbox_coords...)
            # Format with citations
        ```
        
        **Key Features:**
        - Type-safe Python API (no SQL injection)
        - Automatic embedding generation
        - Hybrid search (semantic + keyword)
        - Real-time filtering and ranking
        """)
    
    with impl_tab2:
        st.markdown("""
        **LLM Answer Synthesis (RAG Pattern):**
        
        ```python
        # Build context from search results
        context = "\\n\\n".join([
            f"[Source {i}] {result['doc_name']}, Page {result['page']}\\n{result['text']}"
            for i, result in enumerate(search_results, 1)
        ])
        
        # Construct prompt with instructions
        prompt = f'''You are an expert clinical protocol assistant...
        
        <context>
        {context}
        </context>
        
        <question>
        {user_question}
        </question>
        
        Answer:'''
        
        # Call Cortex AI Complete
        sql = "SELECT SNOWFLAKE.CORTEX.AI_COMPLETE(?, ?) AS response"
        result = session.sql(sql, params=[model_name, prompt]).collect()
        ```
        
        **Available Models:**
        - Claude: claude-4-sonnet, claude-haiku-4-5, claude-sonnet-4-5
        - Llama: llama4-maverick, llama4-scout, llama3.1-405b
        - GPT: openai-gpt-5, openai-gpt-5-mini
        - Mistral: mistral-large2
        """)
    
    with impl_tab3:
        st.markdown("""
        **PDF Processing Pipeline:**
        
        ```sql
        -- 1. PDF Text Extraction UDF
        CREATE FUNCTION pdf_txt_mapper_v3(scoped_file_url STRING)
        RETURNS VARCHAR
        LANGUAGE PYTHON
        PACKAGES = ('pdfminer')
        AS $$
            # Extract text with bounding boxes
            for page_num, page in enumerate(pages, start=1):
                for text_box in page_layout:
                    x0, y0, x1, y1 = text_box.bbox
                    yield {
                        'page': page_num,
                        'bbox': [x0, y0, x1, y1],
                        'page_width': page.width,
                        'page_height': page.height,
                        'txt': text_box.get_text()
                    }
        $$;
        
        -- 2. Structured Storage
        CREATE TABLE document_chunks (
            chunk_id VARCHAR PRIMARY KEY,
            doc_name VARCHAR,
            page INTEGER,
            text VARCHAR,
            bbox_x0 FLOAT, bbox_y0 FLOAT, bbox_x1 FLOAT, bbox_y1 FLOAT,
            page_width FLOAT, page_height FLOAT,
            extracted_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
        );
        
        -- 3. Cortex Search Service
        CREATE CORTEX SEARCH SERVICE protocol_search
        ON text
        ATTRIBUTES page, doc_name
        WAREHOUSE = compute_wh
        EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'
        AS (SELECT * FROM document_chunks);
        ```
        """)
    
    st.divider()
    
    # Cost Estimation
    st.markdown("### üí∞ Cost Estimation")
    
    if st.session_state.performance_metrics['total_searches'] > 0:
        # Rough cost calculation (these are example rates)
        search_cost = st.session_state.performance_metrics['cortex_search_calls'] * 0.001  # $0.001 per search
        llm_cost = (st.session_state.performance_metrics['total_input_tokens'] + 
                   st.session_state.performance_metrics['total_output_tokens']) * 0.00002  # $0.02 per 1K tokens
        total_cost = search_cost + llm_cost
        
        cost_col1, cost_col2, cost_col3 = st.columns(3)
        
        with cost_col1:
            st.metric("Cortex Search", f"${search_cost:.4f}", help="Estimated cost for search operations")
        
        with cost_col2:
            st.metric("AI Complete", f"${llm_cost:.4f}", help="Estimated cost for LLM operations")
        
        with cost_col3:
            st.metric("Total Session", f"${total_cost:.4f}", help="Total estimated cost for this session")
        
        st.caption("üí° **Note:** These are estimated costs based on typical Snowflake Cortex pricing. Actual costs may vary.")
    else:
        st.info("Execute some searches to see cost estimates.")
    
    # Clear metrics button
    if st.button("üîÑ Reset Session Metrics"):
        st.session_state.performance_metrics = {
            'total_searches': 0,
            'total_response_time': 0,
            'cortex_search_calls': 0,
            'llm_calls': 0,
            'total_input_tokens': 0,
            'total_output_tokens': 0
        }
        st.session_state.execution_log = []
        st.success("Session metrics reset!")
        st.rerun()

# ============================================================================
# Footer
# ============================================================================

# Professional footer following Snowflake best practices
st.markdown("---")
st.markdown("""
<div style='text-align: center; padding: 30px; background: linear-gradient(135deg, #F8FAFC 0%, #E8EEF2 100%); border-radius: 12px; margin-top: 40px;'>
    <p style='font-size: 18px; font-weight: 600; color: #1E293B; margin-bottom: 8px;'>
        ‚ùÑÔ∏è Clinical Protocol Intelligence ‚ùÑÔ∏è
    </p>
    <p style='color: #64748b; font-size: 14px; margin-bottom: 0;'>
        Built with Snowflake Cortex AI | Powered by Streamlit in Snowflake
    </p>
    <p style='color: #64748b; font-size: 12px; margin-top: 8px;'>
        üîç Semantic Search + ü§ñ AI Synthesis + üìç Precise Citations
    </p>
</div>
""", unsafe_allow_html=True)

